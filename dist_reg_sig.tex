\documentclass[11pt]{article}
\usepackage{amsmath,amssymb}
\begin{document}

\title{Signature Kernels for Time Series Distribution Regression}
\author{}
\date{}
\maketitle

\section{Introduction}
Distribution regression refers to supervised learning where each input is an entire distribution (a set or bag of instances) rather than a single data point:contentReference[oaicite:0]{index=0}:contentReference[oaicite:1]{index=1}. In the setting of \emph{sequential} data, this means we are given groups of time-series (each group can be viewed as a probability distribution on path space) with associated labels, and we aim to learn a function $F$ mapping each distribution of time-series to a target value:contentReference[oaicite:2]{index=2}:contentReference[oaicite:3]{index=3}. Such problems arise, for example, when predicting a global property of a system (e.g. temperature of a gas) from the collection of trajectories of its particles:contentReference[oaicite:4]{index=4}. This task is challenging because time-series have an inherent \emph{order} and may be irregularly sampled:contentReference[oaicite:5]{index=5}. We require representations of these complex sequential distributions that respect temporal structure and are robust to irregular timing. 

A recent approach addresses this by leveraging the mathematical \textbf{signature} of paths from rough path theory, and its extension, the \textbf{expected signature} for distributions of paths:contentReference[oaicite:6]{index=6}:contentReference[oaicite:7]{index=7}. The signature provides an explicit, rich feature map for individual time-series, and the expected signature lifts this to a feature for probability measures on time-series. In this white paper, we summarize the core methodology of the \emph{signature kernel} approach for time-series regression (and by extension classification) on distributions of sequences. We focus on the mathematical formulation of path signatures and expected signatures, the universal approximation results that justify their use, and the construction of a signature-based kernel that enables practical learning. The aim is to present these as mathematically grounded tools that quantitative researchers can confidently apply.

\section{Path Signatures and Tensor Algebra}
We model a $d$-dimensional time series as a continuous path $x: [a,T] \to \mathbb{R}^d$, assumed for theoretical convenience to be Lipschitz continuous (a broad class encompassing most practical sequences):contentReference[oaicite:8]{index=8}:contentReference[oaicite:9]{index=9}. To define signature features, we first describe the algebraic environment. Let $E = \mathbb{R}^d$ be the state space of a single time-step of the path. The \textbf{tensor algebra} $T(E)$ over $E$ is the direct sum of all tensor powers of $E$:
\[
T(E) \;=\; \mathbb{R} \;\oplus\; E \;\oplus\; (E\otimes E) \;\oplus\; (E^{\otimes 3}) \;\oplus \;\cdots,
\] 
i.e. $T(E) = \bigoplus_{k=0}^{\infty} E^{\otimes k}$. An element of $T(E)$ can be thought of as an infinite formal series $A = (A^0, A^1, A^2, \ldots)$ with $A^k \in E^{\otimes k}$. In particular, for each $k$, if we choose a basis $e_1,\dots,e_d$ of $E$, an element $A^k \in E^{\otimes k}$ can be identified with its coordinates $A_{(i_1\ldots i_k)}$ for multi-index $(i_1,\dots,i_k)\in\{1,\dots,d\}^k$. The tensor algebra $T(E)$ can be endowed with a natural Hilbert space structure by defining an inner product that makes the tensors of different degrees orthogonal and, for each degree $k$, uses the standard Euclidean inner product on $E^{\otimes k}$:contentReference[oaicite:10]{index=10}:contentReference[oaicite:11]{index=11}. In formula terms, if $A=(A^0,A^1,\dots)$ and $B=(B^0,B^1,\dots)$ in $T(E)$, one convenient inner product is 
\[
\langle A, B\rangle_{T(E)} = \sum_{k\ge0} \langle A^k, B^k\rangle_{E^{\otimes k}}, 
\] 
so that $T(E)$ becomes an (infinite-dimensional) Hilbert space:contentReference[oaicite:12]{index=12}:contentReference[oaicite:13]{index=13}. This will serve as our feature space.

The \textbf{signature} of a path is a series of iterated time-ordered integrals capturing the path's sequential structure:contentReference[oaicite:14]{index=14}:contentReference[oaicite:15]{index=15}. For a path $x:[a,T]\to\mathbb{R}^d$, the signature $S(x)$ is defined as the element of $T(E)$ given by 
\begin{equation}
\label{eq:signature-def}
S(x)_{(i_1 \ldots i_k)} \;=\; \int_{a \le t_1 < t_2 < \cdots < t_k \le T} dx_{t_1}^{(i_1)}\,dx_{t_2}^{(i_2)}\cdots dx_{t_k}^{(i_k)},
\end{equation}
for each multi-index $(i_1\ldots i_k)$ of length $k\ge 1$, with $S(x)_{()}=1$ as the 0th-order term:contentReference[oaicite:16]{index=16}:contentReference[oaicite:17]{index=17}. In words, $S(x)$ consists of all iterated integrals of the path coordinates in order. For example, the first-level signature $S(x)_{(i)}=\int_a^T dx_t^{(i)}$ gives the net displacement in coordinate $i$, the second-level $S(x)_{(ij)}=\int_{a\le t_1 < t_2 \le T} dx_{t_1}^{(i)}\,dx_{t_2}^{(j)}$ captures information about area and ordering between coordinates $i$ and $j$, and higher orders encode increasingly complex sequential patterns. Intuitively, $S(x)$ is a principled way to convert a path of arbitrary length into a (potentially infinite-dimensional) feature vector summarizing all its time-ordered moments.

Crucially, the signature transformation is \emph{highly expressive and largely lossless}: under mild conditions, the mapping $x \mapsto S(x)$ is injective (distinct paths have distinct signatures, except for certain pathologies):contentReference[oaicite:18]{index=18}:contentReference[oaicite:19]{index=19}. In fact, the signature is known to capture the complete path up to time-parametrization; if two paths (that are not purely ``tree-like'' in the sense of Hambly–Lyons) share the same signature, they trace out the same shape in $\mathbb{R}^d$:contentReference[oaicite:20]{index=20}. This property gives confidence that no important information about the sequence is lost by using $S(x)$ as features. Furthermore, the coordinates of $S(x)$ serve as a sort of ``basis'' for functions on path space: any continuous functional $f$ on a compact set of paths can be uniformly approximated arbitrarily well by a linear combination of finitely many signature coordinates:contentReference[oaicite:21]{index=21}:contentReference[oaicite:22]{index=22}. In other words, polynomial functions of the signature are dense in the space of continuous functions of the path (an analogue of the Weierstrass approximation theorem for path space):contentReference[oaicite:23]{index=23}:contentReference[oaicite:24]{index=24}. This universality result (see e.g. Theorem 2.1 of \cite{Lemercier2021}) indicates that the signature is an \emph{ideal feature map for paths}:contentReference[oaicite:25]{index=25}, in the sense that any desired dependency on the path can be captured by some combination of signature features.

Another important property is the \textbf{time-parametrization invariance} of signatures. The iterated integrals in \eqref{eq:signature-def} are invariant to any monotonic reparameterization of time (they depend only on the path's geometric shape and ordering, not on the specific speeds or sampling irregularities):contentReference[oaicite:26]{index=26}:contentReference[oaicite:27]{index=27}. For example, if a path is traversed faster or with uneven time steps, its signature remains the same as long as the trajectory through space is identical. This explains the empirical robustness of signature methods to irregularly sampled time-series:contentReference[oaicite:28]{index=28}. If one does need to encode absolute timing information, it can be incorporated by augmenting the path with time as an additional coordinate:contentReference[oaicite:29]{index=29}. Otherwise, the signature naturally abstracts away the sampling schedule and focuses on the intrinsic shape of the sequential data, an appealing property for financial time-series and other scenarios where resampling may occur.

In practice, $S(x)$ is an infinite series, but it is typically truncated at some finite level $n$ to yield a finite-dimensional feature vector $S_{\le n}(x) = (1, S(x)_{(i)}, S(x)_{(i_1 i_2)}, \dots, S(x)_{(i_1\ldots i_n)})$. Thanks to results in rough path theory, the higher-order terms decay in magnitude (roughly factorially fast for bounded variation paths:contentReference[oaicite:30]{index=30}:contentReference[oaicite:31]{index=31}), so truncating the series at a reasonably low level often incurs negligible information loss in practice:contentReference[oaicite:32]{index=32}:contentReference[oaicite:33]{index=33}. This allows computational feasibility while retaining the signature’s expressive power.

\section{Expected Signatures for Distributions on Path Space}
Given that $S(x)$ provides a rich representation of an individual path $x$, we now ask: how can we represent a distribution of paths (a collection of time-series)? Consider a probability measure $\mu$ on path space $C([a,T],\mathbb{R}^d)$ (for example, an empirical distribution placing equal mass on each path in a set):contentReference[oaicite:34]{index=34}:contentReference[oaicite:35]{index=35}. The \textbf{expected signature} of $\mu$, denoted $ES(\mu)$, is defined as the expectation (with respect to $\mu$) of the signature of a random path $X \sim \mu$:contentReference[oaicite:36]{index=36}:
\begin{equation}
\label{eq:expected-signature}
ES(\mu)_{(i_1\ldots i_k)} \;=\; \mathbb{E}_{x \sim \mu}\!\Big[S(x)_{(i_1 \ldots i_k)}\Big] \;=\; \int_{x} S(x)_{(i_1\ldots i_k)}\,\mu(dx),
\end{equation}
for every multi-index $(i_1,\dots,i_k)$:contentReference[oaicite:37]{index=37}. In other words, one simply integrates each coordinate of the path’s signature against the distribution $\mu$. Since $S(x)$ lives in $T(E)$, $ES(\mu)$ also lies in the same tensor space $T(E)$, making it a comparable feature vector for the distribution. The expected signature can be thought of as summarizing all the moments of the distribution of paths in a time-ordered manner, analogous to how the moment-generating function (or cumulants) characterize a conventional probability distribution on $\mathbb{R}^d$.

Remarkably, the expected signature inherits the universality and injectivity properties of the path signature. In fact, it has been proven that (under mild conditions, e.g. compact support), the mapping $\mu \mapsto ES(\mu)$ is injective:contentReference[oaicite:38]{index=38}. This means the entire law of a random path is determined uniquely by its expected signature:contentReference[oaicite:39]{index=39}:contentReference[oaicite:40]{index=40}. No two distinct distributions on path space share the same expected signature representation. This result, due to Chevyrev and Oberhauser (2018) in rough path theory, is an infinite-dimensional analogue of the idea that a probability distribution is determined by its moments or characteristic function. It assures us that using $ES(\mu)$ as a feature for a distribution entails no loss of information (in principle) about that distribution's identity. Additionally, the mapping is weakly continuous:contentReference[oaicite:41]{index=41}, so small perturbations in the probability measure (in the sense of weak convergence) lead to small changes in $ES(\mu)$ in the $T(E)$ norm, an important stability property for learning. Together, these properties position the expected signature as an \emph{appropriate feature map for probability measures on paths}:contentReference[oaicite:42]{index=42}:contentReference[oaicite:43]{index=43}, directly parallel to $S(x)$ being an ideal feature map for individual paths.

The expected signature gives us a way to represent an entire bag of time-series by a single (infinite-dimensional) vector in $T(E)$. We can now consider how to perform learning (regression or classification) using this representation. Conceptually, given training data of pairs $(\mu_i, y_i)$ where $\mu_i$ is the empirical distribution of paths in group $i$ and $y_i$ is the label:contentReference[oaicite:44]{index=44}:contentReference[oaicite:45]{index=45}, one could attempt to fit a functional relationship $y \approx F(\mu)$ by leveraging features derived from $ES(\mu)$. Two broad strategies emerge: a \emph{feature-based approach} that explicitly uses coordinates of $ES(\mu)$ (or related expansions) in a linear model, and a \emph{kernel-based approach} that defines a kernel between distributions via their expected signatures and employs a kernel machine. Both approaches have been proposed in the literature:contentReference[oaicite:46]{index=46}:contentReference[oaicite:47]{index=47}; we outline each in turn.

\section{Learning with Expected Signatures}
\subsection{Feature-Based Approach (SES)}
One approach is to use the expected signature as the basis of feature vectors and perform a (potentially regularized) linear regression in this feature space. A direct idea is to truncate the expected signature \eqref{eq:expected-signature} at some level $m$ and use the components $ES(\mu)_{(i_1\ldots i_k)}$ for $0\le k \le m$ as features. However, to fully exploit temporal information, one can go a step further: consider the distribution $\mu$ over paths as evolving over time. For each time $t \in [a,T]$, consider the partial path up to $t$ and take its expected signature $\mathbb{E}_{x\sim \mu}[S(x|_{[a,t]})]$. As $t$ runs from $a$ to $T$, this yields a path in the tensor space $T(E)$, often called the \emph{pathwise expected signature} $\Phi(\mu): [a,T] \to T(E)$:contentReference[oaicite:48]{index=48}. Intuitively, $\Phi(\mu)$ is a $T(E)$-valued time-series capturing how the distribution’s expected signature accumulates over time. Now we can apply the signature again to this $T(E)$-valued path $\Phi(\mu)$, obtaining $S(\Phi(\mu))$, which is an element of $T(T(E))$. This object $S(\Phi(\mu))$ can be thought of as a collection of features that are effectively “signatures of signatures.” In practice one would truncate both the inner expected signature and the outer signature to some levels $n$ and $m$, respectively.

The key theoretical insight is that using such features is universal for approximating any continuous functional on distributions. In particular, if $F: P(X)\to \mathbb{R}$ is any continuous target function of the distribution (with $X$ a compact set of paths), there exists some finite truncation levels $n,m$ and finitely many coordinates of $S(\Phi(\mu))$ such that $F(\mu)$ can be arbitrarily well-approximated by a linear combination of those coordinates:contentReference[oaicite:49]{index=49}:contentReference[oaicite:50]{index=50}. Formally, Lemercier \emph{et al.} (2021) prove a density result (Theorem 3.2) stating that for every $\varepsilon>0$, we can find coefficients $\alpha_J$ and multi-indices $J$ (which correspond to particular iterated-integral features of $\Phi(\mu)$) such that 
\[
\Big|F(\mu) - \sum_{k=0}^{m}\sum_{J \in \{1,\dots,d\}^k} \alpha_J\; [S(\Phi(\mu))]_{J}\Big| < \varepsilon,
\] 
for all $\mu$ in $P(X)$:contentReference[oaicite:51]{index=51}:contentReference[oaicite:52]{index=52}. In plainer terms, by using enough expected-signature-based features, a linear model can approximate $F$ arbitrarily well on the space of distributions. This result is a direct analogue of the universal approximation property of path signatures, now lifted to distributions. It justifies a simple linear regression on these features as a powerful method for distribution-to-value regression. The authors dub this approach the \emph{signature-of-expected-signature} method (SES):contentReference[oaicite:53]{index=53}.

In implementation, one would compute for each training sample $\mu_i$ the truncated pathwise expected signature $\Phi(\mu_i)$ (which can be done by averaging the signature of each observed path at incremental time steps:contentReference[oaicite:54]{index=54}). Then one computes the signature of that $\Phi(\mu_i)$ (truncated at some level), producing a fixed-length feature vector. Finally, any standard regression or classification algorithm (even a simple linear regression or logistic regression) can be applied to fit the target $y_i$. Thanks to the theoretical guarantees, this linear model in signature features is rich enough to capture complex dependencies. The SES approach is particularly attractive when the total number of individual streams (paths) is large but each path’s dimension $d$ is relatively small:contentReference[oaicite:55]{index=55}, because truncating at a moderate level $m$ yields a manageable number of features (which grows exponentially in $d$). In such regimes, SES provides a computationally feasible and statistically powerful solution.

\subsection{Kernel-Based Approach (KES)}
An alternative approach avoids explicit feature truncation and instead uses a kernel to implicitly handle potentially infinite-dimensional feature mappings. The idea is to define a positive-definite kernel between two distributions $\mu$ and $\nu$ based on their expected signatures. A natural choice is to take the squared distance between $ES(\mu)$ and $ES(\nu)$ in $T(E)$ and convert it into a Gaussian radial basis function kernel. Concretely, one defines:
\begin{equation}
\label{eq:kernel}
k(\mu,\nu) \;=\; \exp\!\Big(-\tfrac{\sigma^2}{2}\,\|ES(\mu) - ES(\nu)\|^2_{T(E)}\Big),
\end{equation}
for some bandwidth parameter $\sigma>0$. This kernel $k: P(X)\times P(X)\to \mathbb{R}$ compares distributions on path space by comparing their expected signature vectors. Because the expected signature is an injective embedding of the distribution (as discussed above), this Gaussian kernel is extremely expressive. In fact, by applying a standard result from kernel theory:contentReference[oaicite:56]{index=56}:contentReference[oaicite:57]{index=57}, one can show that $k(\mu,\nu)$ is a \emph{universal kernel} on $P(X)$. Universality here means that the reproducing kernel Hilbert space (RKHS) associated with $k$ is dense in $C(P(X))$, the space of continuous real-valued functions on the compact set of distributions $P(X)$:contentReference[oaicite:58]{index=58}:contentReference[oaicite:59]{index=59}. Equivalently, any continuous functional of the distribution can be approximated arbitrarily well by some $k$-kernel machine (such as a support vector machine or Gaussian process) using $k$. This is precisely Theorem 3.3 of \cite{Lemercier2021}, and its proof hinges on the fact that $ES: P(X)\to T(E)$ is a continuous injective map into a separable Hilbert space, which by a theorem of Steinwart implies Gaussian kernels on that feature space are universal:contentReference[oaicite:60]{index=60}:contentReference[oaicite:61]{index=61}.

Practically, the kernel approach (referred to as KES in the original paper) means one can use any kernelized learning algorithm (e.g. kernel ridge regression, SVM, Gaussian process regression/classification) with the kernel $k(\mu,\nu)$ defined in \eqref{eq:kernel}. The challenge is computing $k(\mu,\nu)$ efficiently. Expanding the definition: 
\[
\|ES(\mu) - ES(\nu)\|^2_{T(E)} = \langle ES(\mu), ES(\mu)\rangle + \langle ES(\nu), ES(\nu)\rangle - 2\langle ES(\mu), ES(\nu)\rangle.
\] 
Now $\langle ES(\mu), ES(\mu)\rangle = \mathbb{E}_{x,x'\sim \mu}\langle S(x), S(x')\rangle_{T(E)}$, which by linearity and Fubini’s theorem can be written as an average over pairs of paths in $\mu$. Similarly for the other terms:contentReference[oaicite:62]{index=62}:contentReference[oaicite:63]{index=63}. Thus each inner product $\langle ES(\mu), ES(\nu)\rangle$ reduces to an expectation of $\langle S(x), S(x')\rangle$ over $x\sim\mu, x'\sim\nu$. The quantity $\langle S(x), S(x')\rangle_{T(E)}$ for two specific paths $x$ and $x'$ is known as the \textbf{signature kernel} $k_{\mathrm{sig}}(x,x')$:contentReference[oaicite:64]{index=64}. Notably, recent work has shown that this kernel (which is essentially a series of iterated integrals comparing two paths) can be computed as the solution to a certain system of linear differential equations (a PDE):contentReference[oaicite:65]{index=65}:contentReference[oaicite:66]{index=66}. In fact, for piecewise linear or controlled paths one can derive explicit computations, and in general one can numerically solve the PDE to obtain $k_{\mathrm{sig}}(x,x')$:contentReference[oaicite:67]{index=67}:contentReference[oaicite:68]{index=68}. This provides a kind of ``kernel trick'' for our distribution kernel: rather than explicitly computing potentially large feature vectors, one can compute each entry $k(\mu_i,\mu_j)$ of the Gram matrix by double-averaging signature kernels between all pairs of constituent paths of $\mu_i$ and $\mu_j$:contentReference[oaicite:69]{index=69}:contentReference[oaicite:70]{index=70}. Algorithms have been proposed to do this efficiently using the PDE trick (see Cass et al., 2020):contentReference[oaicite:71]{index=71}. The end result is that we can evaluate the kernel matrix for $\{\mu_i\}$ and plug it into any kernelized learner.

The KES approach is particularly suited to scenarios where individual paths are high-dimensional (large $d$) but the total number of path samples is not huge:contentReference[oaicite:72]{index=72}. In such cases, the feature-based approach would require truncating at a very low level to avoid an explosion of feature dimension (since the number of signature features grows rapidly with $d$ and degree). The kernel method sidesteps explicitly enumerating features in the high-dimensional case, relying on the kernel evaluation which cleverly sums over all orders in an implicit way. On the other hand, if one has extremely many sequences but each of small dimension, the feature method (SES) may be more computationally convenient:contentReference[oaicite:73]{index=73}. In either regime, both approaches enjoy the same strong theoretical guarantees of universality and consistency, thanks to the expected signature representation.

\section{Discussion and Conclusion}
We have outlined the signature kernel methodology for learning on distributions of sequential data, highlighting how the path signature and expected signature together provide a principled representation for complex time-series data with solid theoretical foundations. The path signature $S(x)$ serves as an injective, invariant, and universal feature map for individual time-series:contentReference[oaicite:74]{index=74}:contentReference[oaicite:75]{index=75}, while the expected signature $ES(\mu)$ lifts this expressiveness to the space of probability distributions on paths, fully capturing distributional differences in a comparable feature in $T(E)$:contentReference[oaicite:76]{index=76}:contentReference[oaicite:77]{index=77}. These constructions are grounded in the algebra of iterated integrals and the tensor Hilbert space, ensuring that the resulting features and kernels are not ad-hoc but rather derived from established stochastic analysis (rough path) theory.

For quantitative researchers, an appealing aspect of this approach is its \textbf{universality}: any continuous functional of the input data (paths or distributions of paths) can in principle be approximated to arbitrary accuracy using linear functionals of the signature features:contentReference[oaicite:78]{index=78}:contentReference[oaicite:79]{index=79} or, equivalently, by a machine learning model in the RKHS of the signature-based kernel:contentReference[oaicite:80]{index=80}:contentReference[oaicite:81]{index=81}. This gives a theoretical guarantee of expressiveness, analogous to universal approximation theorems in neural networks but here with an explicit feature map and kernel. Moreover, the signature features are \textbf{robust to time-warping and irregular sampling}, meaning the model performance will not degrade if time-series are observed at irregular intervals or under different parameterizations:contentReference[oaicite:82]{index=82}. This is particularly important in financial or scientific data where measurement times can be uneven. If time itself carries information, the framework easily accommodates that by augmenting paths with a time channel:contentReference[oaicite:83]{index=83}.

In summary, the signature and expected signature methodology provides a mathematically elegant and powerful toolkit for regression and classification on complex sequential data. By representing each time-series path as a series of iterated integrals and each distribution of paths as the expectation of those integrals, we obtain feature representations that are provably rich enough to approximate any desired outcome and are endowed with invariances desirable in practice. The combination of a feature-based linear method (SES) and a kernel-based method (KES) offers flexibility to handle different data scales and dimensions:contentReference[oaicite:84]{index=84}, both underpinned by the same theoretical assurances. These methods have already shown state-of-the-art performance in diverse applications (finance, thermodynamics, agriculture) while maintaining interpretability and rigor:contentReference[oaicite:85]{index=85}:contentReference[oaicite:86]{index=86}. We encourage quants and researchers to consider the signature approach as a principled alternative to ad-hoc feature engineering for sequential data – one that comes with the confidence of universal approximation capabilities and robustness, grounded in decades of development in rough path theory.

\bibliographystyle{plain}
\begin{thebibliography}{9}\setlength{\itemsep}{-1mm}
\bibitem{Lemercier2021} M. Lemercier, C. Salvi, T. Damoulas, E. V. Bonilla, and T. Lyons (2021). \textit{Distribution Regression for Sequential Data}. AISTATS 2021.  :contentReference[oaicite:87]{index=87}:contentReference[oaicite:88]{index=88}
\bibitem{Lyons2014} T. Lyons (2014). \textit{Rough paths, signatures and the modelling of functions}. Proceedings of the International Congress of Mathematicians (ICM 2014). :contentReference[oaicite:89]{index=89}
\bibitem{Chevyrev2018} I. Chevyrev and H. Oberhauser (2018). \textit{Signature moments to characterize laws of stochastic processes}. Annals of Probability 46(3). :contentReference[oaicite:90]{index=90}:contentReference[oaicite:91]{index=91}
\bibitem{Cass2019} T. Cass, C. Litterer, and T. Lyons (2020). \textit{Nothing is expected in the expected signature kernel}. arXiv:2001.00706. :contentReference[oaicite:92]{index=92}:contentReference[oaicite:93]{index=93}
\bibitem{KiralyaOberhauser2019} F. Király and H. Oberhauser (2019). \textit{Kernels for sequentially ordered data}. Machine Learning 108(2). :contentReference[oaicite:94]{index=94}
\end{thebibliography}

\end{document}
