
\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, graphicx, hyperref}
\usepackage{geometry}
\geometry{margin=1in}

\title{Quantum Entropy Scoring for Outlier Detection in High Dimensions}
\author{Ali Fathi \\ CIB Data Science}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Detecting outliers in high-dimensional data sets remains one of the most persistent challenges in modern data analysis. Traditional approaches typically rely either on distance-based scoring, which measures how far a sample lies from the empirical mean, or on spectral methods, which identify directions of inflated variance in the data. Each of these strategies has its own strengths, but also clear weaknesses when confronted with heterogeneous or subtle outliers. This white paper provides a focused exposition of Quantum Entropy (QUE) scoring, a recently proposed method that interpolates between the two extremes by introducing an entropy-regularized spectral mixture. The result is a scoring rule that remains sensitive to multiple forms of deviation and that scales efficiently to very high dimensions.
\end{abstract}

\section{Outlier Detection in High Dimensions}
Outliers in a data set may arise for many different reasons. Some points may have unusually large magnitude, standing out purely by virtue of their distance from the center of mass of the distribution. Others may not appear extreme in isolation, but when taken collectively they bias the empirical covariance, creating spurious directions of variance that are not representative of the underlying population. In real data, both types often coexist, with corrupted samples distributed across several unrelated directions.

The difficulty with conventional methods is that each is tuned to only one of these cases. Distance-based scoring excels at identifying extreme points with inflated norms, but misses those that are directionally biased yet norm-preserving. Conversely, spectral approaches focus attention narrowly on the top eigenvector of the empirical covariance, but this is not sufficient if adversarial outliers distribute their influence across several orthogonal directions. In such cases the method must be repeated multiple times, incurring substantial computational overhead, while still remaining blind to more diffuse corruption patterns.

\section{Quantum Entropy Scores}
Quantum Entropy (QUE) scoring was introduced to overcome precisely this gap. The central object in QUE is a density matrix
\[
U = \frac{\exp(\alpha \Sigma)}{\mathrm{tr} \exp(\alpha \Sigma)},
\]
constructed from the empirical covariance $\Sigma$ of the data. Here the parameter $\alpha > 0$ regulates the trade-off between focusing mass on a few high-variance directions and spreading mass more evenly across the spectrum. The exponential and normalization make $U$ a bona fide probability distribution over eigen-directions, with the von Neumann entropy serving as a regularizer.

Given this $U$, the outlier score of a data point $X_i$ is defined as
\[
\tau_i = (X_i - \mu)^\top U (X_i - \mu),
\]
where $\mu$ is the empirical mean. The intuition is that a sample is penalized whenever it contributes to inflated variance in any direction, not merely along the single top eigenvector. When $\alpha$ is taken close to zero, the matrix $U$ approaches the scaled identity, and the score reduces to the familiar squared Euclidean norm. When $\alpha$ grows large, $U$ concentrates on the leading eigenvector, and the score becomes essentially a projection along the most biased direction. For intermediate values of $\alpha$, the scoring rule interpolates smoothly between these two behaviors, yielding sensitivity to both types of anomalies simultaneously.

\section{Scalability and Implementation}
A key advantage of QUE is computational. Naively computing the matrix exponential appears costly, but by combining Johnsonâ€“Lindenstrauss sketching with truncated series expansions one can obtain $(1\pm \varepsilon)$ approximations to the scores in nearly-linear time in the input size. This makes it practical to apply QUE scoring on data sets with thousands of dimensions, where traditional repeated eigen-decomposition would be prohibitive.

\section{Illustrative Example}
The subtlety of high-dimensional outliers is illustrated in Figure~\ref{fig:que-example}. The blue circle represents a purely norm-based view: every point is judged by its distance from the mean. From this perspective, many corrupted samples appear perfectly innocuous, since their norms are indistinguishable from those of true inliers.

However, when an $\varepsilon$-fraction of such samples are all aligned in a similar direction, their aggregate effect is to inflate variance along that axis. This is captured by the red ellipse, which indicates how the empirical covariance is distorted. Importantly, no single corrupted point stands out as extreme, yet together they introduce a directional bias large enough to shift the mean and create a spurious eigenvalue.

The key insight is that outliers need not look anomalous in isolation: their influence may only be visible in the geometry of the data as a whole. Quantum Entropy Scoring is designed precisely for this setting. By interpolating between distance-based scoring and spectral analysis, it detects when many seemingly normal points collectively bias the covariance structure, without being restricted to just the top eigenvector.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.65\textwidth]{mean_shift1.png}
    \caption{Schematic illustrating collective corruption. While each corrupted point (red) looks normal in norm, their aggregate effect inflates variance along a direction, distorting the covariance (red ellipse) and shifting the mean. Norm-based methods (blue circle) fail to notice this, while QUE scoring captures the distortion by blending norm and spectral information.}
    \label{fig:que-example}
\end{figure}

\section{Challenger Methods}
In order to assess the merits of Quantum Entropy (QUE) scoring, we benchmark against several standard
outlier detection methods that represent distinct statistical principles. Each method assigns a score
$\tau_i$ to every sample $X_i \in \mathbb{R}^d$, with larger values indicating higher likelihood of being
an outlier.

\subsection{Euclidean Norm Scoring}
The most direct approach is to use the squared Euclidean distance from the empirical mean:
\[
\tau_i^{\ell_2} = \|X_i - \mu\|_2^2,
\qquad \mu = \frac{1}{n}\sum_{j=1}^n X_j.
\]
This captures gross deviations in magnitude, but fails when corruptions are aligned in a direction without
large changes in norm.

\subsection{Naive Spectral Scoring}
Spectral methods focus on directions of inflated variance. Let $v$ be the top eigenvector of the empirical
covariance matrix $\Sigma = \tfrac{1}{n}\sum_{j=1}^n (X_j - \mu)(X_j - \mu)^\top$. The score is
\[
\tau_i^{\text{spec}} = \langle X_i - \mu, v \rangle^2.
\]
This method detects coordinated outliers aligned with a dominant direction, but misses diffuse corruption
spread across several orthogonal directions.

\subsection{Isolation Forest}
Isolation Forests are ensemble tree-based methods that recursively partition the data space. Outliers are
expected to be isolated in fewer random splits, leading to shorter path lengths in the trees. If
$h(X_i)$ denotes the average path length of $X_i$ across the ensemble, the score is inversely related:
\[
\tau_i^{\text{IF}} \propto \frac{1}{h(X_i)}.
\]
This approach is distribution-agnostic and works well for samples that can be easily separated by recursive
partitioning.

\subsection{Local Outlier Factor (LOF)}
The Local Outlier Factor compares the local density around a point to the densities around its neighbors.
Given $k$-nearest neighbor distance $d_k(X_i)$, the local reachability density is
\[
\text{lrd}(X_i) = \left( \frac{1}{k} \sum_{X_j \in N_k(X_i)} \max\{d_k(X_j), \|X_i - X_j\|\} \right)^{-1}.
\]
The LOF score is then
\[
\tau_i^{\text{LOF}} = \frac{1}{k}\sum_{X_j \in N_k(X_i)} \frac{\text{lrd}(X_j)}{\text{lrd}(X_i)}.
\]
A high value indicates that $X_i$ lies in a region of significantly lower density than its neighbors.

\subsection{$k$-Nearest Neighbor Distance}
A simpler alternative to LOF is to score points directly by their distance to the $k$-th nearest neighbor:
\[
\tau_i^{k\text{-NN}} = d_k(X_i).
\]
Outliers are expected to lie farther from their neighbors, leading to larger values of $\tau_i^{k\text{-NN}}$.

\subsection{Elliptic Envelope}
Assuming that inliers follow a Gaussian distribution, one can fit a robust covariance estimator
$\hat{\mu}, \hat{\Sigma}$ and compute the Mahalanobis distance
\[
\tau_i^{\text{EE}} = (X_i - \hat{\mu})^\top \hat{\Sigma}^{-1} (X_i - \hat{\mu}).
\]
This approach is effective when the inlier distribution is close to elliptical but degrades if the
distribution is multi-modal or heavy-tailed.

\medskip
Together, these baselines provide a diverse set of challengers: norm-based, spectral, density-based,
tree-based, and covariance-based. This diversity ensures that QUE is evaluated not just against trivial
rules, but against widely-used outlier detection paradigms.


\end{document}
