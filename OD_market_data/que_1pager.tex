
\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, graphicx, hyperref}
\usepackage{geometry}
\geometry{margin=1in}

\title{Quantum Entropy Scoring for Outlier Detection in High Dimensions}
\author{Ali Fathi \\ CIB Data Science}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Detecting outliers in high-dimensional data sets remains one of the most persistent challenges in modern data analysis. Traditional approaches typically rely either on distance-based scoring, which measures how far a sample lies from the empirical mean, or on spectral methods, which identify directions of inflated variance in the data. Each of these strategies has its own strengths, but also clear weaknesses when confronted with heterogeneous or subtle outliers. This white paper provides a focused exposition of Quantum Entropy (QUE) scoring, a recently proposed method that interpolates between the two extremes by introducing an entropy-regularized spectral mixture. The result is a scoring rule that remains sensitive to multiple forms of deviation and that scales efficiently to very high dimensions.
\end{abstract}

\section{Outlier Detection in High Dimensions}
Outliers in a data set may arise for many different reasons. Some points may have unusually large magnitude, standing out purely by virtue of their distance from the center of mass of the distribution. Others may not appear extreme in isolation, but when taken collectively they bias the empirical covariance, creating spurious directions of variance that are not representative of the underlying population. In real data, both types often coexist, with corrupted samples distributed across several unrelated directions.

The difficulty with conventional methods is that each is tuned to only one of these cases. Distance-based scoring excels at identifying extreme points with inflated norms, but misses those that are directionally biased yet norm-preserving. Conversely, spectral approaches focus attention narrowly on the top eigenvector of the empirical covariance, but this is not sufficient if adversarial outliers distribute their influence across several orthogonal directions. In such cases the method must be repeated multiple times, incurring substantial computational overhead, while still remaining blind to more diffuse corruption patterns.

\section{Quantum Entropy Scores}
Quantum Entropy (QUE) scoring was introduced to overcome precisely this gap. The central object in QUE is a density matrix
\[
U = \frac{\exp(\alpha \Sigma)}{\mathrm{tr} \exp(\alpha \Sigma)},
\]
constructed from the empirical covariance $\Sigma$ of the data. Here the parameter $\alpha > 0$ regulates the trade-off between focusing mass on a few high-variance directions and spreading mass more evenly across the spectrum. The exponential and normalization make $U$ a bona fide probability distribution over eigen-directions, with the von Neumann entropy serving as a regularizer.

Given this $U$, the outlier score of a data point $X_i$ is defined as
\[
\tau_i = (X_i - \mu)^\top U (X_i - \mu),
\]
where $\mu$ is the empirical mean. The intuition is that a sample is penalized whenever it contributes to inflated variance in any direction, not merely along the single top eigenvector. When $\alpha$ is taken close to zero, the matrix $U$ approaches the scaled identity, and the score reduces to the familiar squared Euclidean norm. When $\alpha$ grows large, $U$ concentrates on the leading eigenvector, and the score becomes essentially a projection along the most biased direction. For intermediate values of $\alpha$, the scoring rule interpolates smoothly between these two behaviors, yielding sensitivity to both types of anomalies simultaneously.

\section{Scalability and Implementation}
A key advantage of QUE is computational. Naively computing the matrix exponential appears costly, but by combining Johnson–Lindenstrauss sketching with truncated series expansions one can obtain $(1\pm \varepsilon)$ approximations to the scores in nearly-linear time in the input size. This makes it practical to apply QUE scoring on data sets with thousands of dimensions, where traditional repeated eigen-decomposition would be prohibitive.

\section{Illustrative Example}
The subtlety of high-dimensional outliers is illustrated in Figure~\ref{fig:que-example}. The blue circle represents a purely norm-based view: every point is judged by its distance from the mean. From this perspective, many corrupted samples appear perfectly innocuous, since their norms are indistinguishable from those of true inliers.

However, when an $\varepsilon$-fraction of such samples are all aligned in a similar direction, their aggregate effect is to inflate variance along that axis. This is captured by the red ellipse, which indicates how the empirical covariance is distorted. Importantly, no single corrupted point stands out as extreme, yet together they introduce a directional bias large enough to shift the mean and create a spurious eigenvalue.

The key insight is that outliers need not look anomalous in isolation: their influence may only be visible in the geometry of the data as a whole. Quantum Entropy Scoring is designed precisely for this setting. By interpolating between distance-based scoring and spectral analysis, it detects when many seemingly normal points collectively bias the covariance structure, without being restricted to just the top eigenvector.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.65\textwidth]{mean_shift1.png}
    \caption{Schematic illustrating collective corruption. While each corrupted point (red) looks normal in norm, their aggregate effect inflates variance along a direction, distorting the covariance (red ellipse) and shifting the mean. Norm-based methods (blue circle) fail to notice this, while QUE scoring captures the distortion by blending norm and spectral information.}
    \label{fig:que-example}
\end{figure}

\section{Challenger Methods}
In order to assess the merits of Quantum Entropy (QUE) scoring, we benchmark against several standard
outlier detection methods that represent distinct statistical principles. Each method assigns a score
$\tau_i$ to every sample $X_i \in \mathbb{R}^d$, with larger values indicating higher likelihood of being
an outlier.

\subsection{Euclidean Norm Scoring}
The most direct approach is to use the squared Euclidean distance from the empirical mean:
\[
\tau_i^{\ell_2} = \|X_i - \mu\|_2^2,
\qquad \mu = \frac{1}{n}\sum_{j=1}^n X_j.
\]
This captures gross deviations in magnitude, but fails when corruptions are aligned in a direction without
large changes in norm.

\subsection{Naive Spectral Scoring}
Spectral methods focus on directions of inflated variance. Let $v$ be the top eigenvector of the empirical
covariance matrix $\Sigma = \tfrac{1}{n}\sum_{j=1}^n (X_j - \mu)(X_j - \mu)^\top$. The score is
\[
\tau_i^{\text{spec}} = \langle X_i - \mu, v \rangle^2.
\]
This method detects coordinated outliers aligned with a dominant direction, but misses diffuse corruption
spread across several orthogonal directions.

\subsection{Isolation Forest}
Isolation Forests are ensemble tree-based methods that recursively partition the data space. Outliers are
expected to be isolated in fewer random splits, leading to shorter path lengths in the trees. If
$h(X_i)$ denotes the average path length of $X_i$ across the ensemble, the score is inversely related:
\[
\tau_i^{\text{IF}} \propto \frac{1}{h(X_i)}.
\]
This approach is distribution-agnostic and works well for samples that can be easily separated by recursive
partitioning.

\subsection{Local Outlier Factor (LOF)}
The Local Outlier Factor compares the local density around a point to the densities around its neighbors.
Given $k$-nearest neighbor distance $d_k(X_i)$, the local reachability density is
\[
\text{lrd}(X_i) = \left( \frac{1}{k} \sum_{X_j \in N_k(X_i)} \max\{d_k(X_j), \|X_i - X_j\|\} \right)^{-1}.
\]
The LOF score is then
\[
\tau_i^{\text{LOF}} = \frac{1}{k}\sum_{X_j \in N_k(X_i)} \frac{\text{lrd}(X_j)}{\text{lrd}(X_i)}.
\]
A high value indicates that $X_i$ lies in a region of significantly lower density than its neighbors.

\subsection{$k$-Nearest Neighbor Distance}
A simpler alternative to LOF is to score points directly by their distance to the $k$-th nearest neighbor:
\[
\tau_i^{k\text{-NN}} = d_k(X_i).
\]
Outliers are expected to lie farther from their neighbors, leading to larger values of $\tau_i^{k\text{-NN}}$.

\subsection{Elliptic Envelope}
Assuming that inliers follow a Gaussian distribution, one can fit a robust covariance estimator
$\hat{\mu}, \hat{\Sigma}$ and compute the Mahalanobis distance
\[
\tau_i^{\text{EE}} = (X_i - \hat{\mu})^\top \hat{\Sigma}^{-1} (X_i - \hat{\mu}).
\]
This approach is effective when the inlier distribution is close to elliptical but degrades if the
distribution is multi-modal or heavy-tailed.

\medskip
Together, these baselines provide a diverse set of challengers: norm-based, spectral, density-based,
tree-based, and covariance-based. This diversity ensures that QUE is evaluated not just against trivial
rules, but against widely-used outlier detection paradigms.

\section{Optimization Objective Underlying QUE}
The central mathematical object in Quantum Entropy (QUE) scoring is the density matrix $U$, which is
obtained as the solution to an entropy-regularized convex program. Given the empirical covariance
matrix $\Sigma \in \mathbb{R}^{d \times d}$, QUE defines $U$ as the optimizer of
\begin{equation}
    \max_{U \succeq 0,\; \mathrm{tr}(U) = 1}
    \; \alpha \,\langle U, \Sigma \rangle + S(U),
    \label{eq:que-opt}
\end{equation}
where $\alpha > 0$ is a trade-off parameter, $\langle A,B \rangle = \mathrm{tr}(A B^\top)$ is the trace
inner product, and
\[
    S(U) = -\langle U, \log U \rangle
\]
is the \emph{quantum entropy} (or von Neumann entropy) of $U$. The constraints enforce that $U$ is a
positive semidefinite matrix with unit trace, i.e., a density matrix.

\subsection{Interpretation}
If $U$ has eigendecomposition
\[
    U = \sum_{i=1}^d \mu_i v_i v_i^\top,
\]
then $\{\mu_i\}$ form a probability distribution over orthonormal directions $\{v_i\}$, since
$\mu_i \ge 0$ and $\sum_i \mu_i = 1$. Under this interpretation:
\[
    \langle U, \Sigma \rangle = \mathbb{E}_{v \sim \mu} \left[ v^\top \Sigma v \right],
\]
is the expected variance of the data in a randomly sampled direction, while $S(U)$ measures the entropy
of the distribution $\mu$. The optimization problem \eqref{eq:que-opt} therefore balances two competing
objectives: concentrate on directions with large variance (driven by outliers) versus maintain high
entropy, i.e., spread mass across multiple directions.

\subsection{Closed Form Solution}
Problem \eqref{eq:que-opt} admits a closed-form solution:
\[
    U^\star = \frac{\exp(\alpha \Sigma)}{\mathrm{tr}\!\left( \exp(\alpha \Sigma) \right)}.
\]
This is obtained by taking the first-order optimality conditions of the Lagrangian
\[
    \mathcal{L}(U,\lambda) = \alpha \langle U, \Sigma \rangle - \langle U, \log U \rangle
    + \lambda \big( 1 - \mathrm{tr}(U) \big),
\]
and noting that the derivative with respect to $U$ yields
\[
    \alpha \Sigma - \log U - I + \lambda I = 0
    \quad \implies \quad
    U \propto \exp(\alpha \Sigma).
\]
Normalization by $\mathrm{tr}(\cdot)$ enforces the unit-trace constraint.

\subsection{Limiting Regimes}
The parameter $\alpha$ controls the degree of concentration:
\begin{itemize}
    \item As $\alpha \to 0$, we have $U \to \tfrac{1}{d} I$, i.e. the uniform distribution over
    all directions, reducing QUE to $\ell_2$ norm scoring.
    \item As $\alpha \to \infty$, $U$ concentrates on the eigenvector corresponding to the largest
    eigenvalue of $\Sigma$, recovering naive spectral scoring.
\end{itemize}
Thus, QUE interpolates continuously between purely norm-based and purely spectral methods, with
intermediate values of $\alpha$ capturing multi-directional outliers.

\subsection{Optimization Perspective}
The entropy term is crucial: without $S(U)$, the problem degenerates into maximizing variance in the
single top direction. The entropy acts as a regularizer, encouraging exploration of multiple
eigen-directions simultaneously. This makes QUE robust against heterogeneous outliers, since any
direction in which the empirical covariance is spuriously inflated contributes to the solution.
Moreover, because the optimization is convex with a closed-form solution, computing approximate QUE
scores is feasible in nearly linear time using randomized linear algebra (e.g.\ Johnson–Lindenstrauss
sketches and truncated series expansion of $\exp(\alpha \Sigma)$).

\section{Synthetic Dataset Construction}

To evaluate Quantum Entropy scoring in a controlled environment, we construct synthetic
high-dimensional datasets that contain both inliers and deliberately injected outliers.
The inliers are sampled independently from an isotropic Gaussian distribution
\[
X_{\text{inlier}} \sim \mathcal{N}(0, I_d),
\]
where $d$ is the ambient feature dimension. These points are centered at the origin
and exhibit spherical covariance.

Outliers are introduced in $k$ distinct directions. For each direction $e_i$ in the
standard basis, we generate a collection of samples from a shifted Gaussian
\[
X_{\text{outlier}} \sim \tfrac{1}{2}\,\mathcal{N}\!\left(+C\sqrt{\tfrac{k}{\varepsilon}}\,e_i,\,
\sigma^2 I_d\right) + \tfrac{1}{2}\,\mathcal{N}\!\left(-C\sqrt{\tfrac{k}{\varepsilon}}\,e_i,\,
\sigma^2 I_d\right),
\]
where $\varepsilon$ is the target corruption rate, $C$ is a scaling constant chosen
so that the contribution of the outliers produces an inflated eigenvalue, and $\sigma$
controls the amount of isotropic noise around each shifted mean.
By design, these outliers have comparable norms to the inliers, so that they do not
appear anomalous in isolation. Their effect is only visible in aggregate: an
$\varepsilon$-fraction of such points is sufficient to distort the empirical covariance
and bias the sample mean.

The final synthetic dataset is the union of inliers and outliers,
\[
X = (1-\varepsilon)\,X_{\text{inlier}} \;\cup\; \varepsilon\,X_{\text{outlier}},
\]
with the total sample size $n$ adjusted so that the fraction of outliers matches the
prescribed $\varepsilon$. This construction is particularly useful for benchmarking
because it stresses the limitations of purely norm-based methods (which cannot
distinguish the outliers individually) and highlights the need for algorithms like
QUE scoring that can capture collective covariance distortions.


\end{document}
