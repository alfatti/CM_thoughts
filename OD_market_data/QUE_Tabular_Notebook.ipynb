{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c641ecb6",
   "metadata": {},
   "source": [
    "# Quantum Entropy (QUE) Scoring & Baselines â€” Tabular Data Notebook\n",
    "\n",
    "This notebook contains only code copied verbatim from the authors' repository to compute **QUE scores** and baseline outlier detectors. To use on *any tabular data*:\n",
    "\n",
    "1. Prepare a tensor `X` (`torch.FloatTensor`) of shape `(n_samples, n_features)` on the appropriate device (`utils.device`).\n",
    "2. (Optional) Center `X` by subtracting its mean column-wise.\n",
    "3. Run `compute_tau1_tau0(X, opt)` to obtain QUE (`tau1`) and naive spectral (`tau0`) scores.\n",
    "4. For baselines, call `l2(X)`, `isolation_forest(X)`, `knn_dist_lof(X)`, `ellenv(X)`.\n",
    "\n",
    "**Note:** All functions below are pasted verbatim from the authors' `mean.py`, `utils.py`, and `baselines.py`. No new algorithmic code has been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895cccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports (copied from authors' scripts)\n",
    "import matplotlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.covariance\n",
    "import sklearn.cluster\n",
    "import random\n",
    "import utils\n",
    "import pdb\n",
    "device = utils.device  # from utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37449cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities (copied verbatim from utils.py)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def cov(X):\n",
    "    #X_mean = X.mean()\n",
    "    X = X - X.mean(dim=0, keepdim=True)\n",
    "\n",
    "    cov = torch.mm(X.t(), X) / X.size(0)\n",
    "    return cov\n",
    "    \n",
    "########################\n",
    "\n",
    "\n",
    "def dominant_eval_cov(X):\n",
    "    n_data = X.size(0)\n",
    "    X = X - X.mean(dim=0, keepdim=True)\n",
    "    X_t = X.t()\n",
    "    X_t_scaled = X_t/n_data\n",
    "    n_round = 5\n",
    "    \n",
    "    v = torch.randn(X.size(-1), 1, device=X.device)\n",
    "    for _ in range(n_round):\n",
    "        v = torch.mm(X_t_scaled, torch.mm(X, v))\n",
    "        #scale each time instead of at the end to avoid overflow\n",
    "        #v = v / (v**2).sum().sqrt()\n",
    "    v = v / (v**2).sum().sqrt()\n",
    "    mu = torch.mm(v.t(), torch.mm(X_t_scaled, torch.mm(X, v))) / (v**2).sum()\n",
    "    \n",
    "    return mu.item(), v.view(-1)\n",
    "'''\n",
    "dominant eval of matrix X\n",
    "Returns: top eval and evec\n",
    "'''\n",
    "\n",
    "def pad_to_2power(X):\n",
    "    n_data, feat_dim = X.size(0), X.size(-1)\n",
    "    power = int(math.ceil(math.log(feat_dim, 2)))\n",
    "    power_diff = 2**power-feat_dim\n",
    "    if power_diff == 0:\n",
    "        return X\n",
    "    padding = torch.zeros(n_data, power_diff, dtype=X.dtype, device=X.device)\n",
    "    X = torch.cat((X, padding), dim=-1)\n",
    "    \n",
    "    return X\n",
    "\n",
    "'''\n",
    "Find dominant eval of XX^t (and evec in the process) using the power method.\n",
    "Without explicitly forming XX^t\n",
    "Returns:\n",
    "-dominant eval + corresponding eigenvector\n",
    "'''\n",
    "\n",
    "def dist_rank(data_x, k, data_y=None, largest=False, opt=None, include_self=False):\n",
    "\n",
    "    if isinstance(data_x, np.ndarray):\n",
    "        data_x = torch.from_numpy(data_x)\n",
    "\n",
    "    if data_y is None:\n",
    "        data_y = data_x\n",
    "    else:\n",
    "        if isinstance(data_y, np.ndarray):\n",
    "            data_y = torch.from_numpy(data_y)\n",
    "    k0 = k\n",
    "    device_o = data_x.device\n",
    "    data_x = data_x.to(device)\n",
    "    data_y = data_y.to(device)\n",
    "    \n",
    "    (data_x_len, dim) = data_x.size()\n",
    "    data_y_len = data_y.size(0)\n",
    "    #break into chunks. 5e6  is total for MNIST point size\n",
    "    #chunk_sz = int(5e6 // data_y_len)\n",
    "    chunk_sz = 16384\n",
    "    chunk_sz = 500 #700 mem error. 1 mil points\n",
    "    if data_y_len > 990000:\n",
    "        chunk_sz = 600 #1000 if over 1.1 mil\n",
    "        #chunk_sz = 500 #1000 if over 1.1 mil \n",
    "    else:\n",
    "        chunk_sz = 3000    \n",
    "\n",
    "    if k+1 > len(data_y):\n",
    "        k = len(data_y) - 1\n",
    "    #if opt is not None and opt.sift:\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        dist_mx = torch.cuda.LongTensor(data_x_len, k+1)\n",
    "        act_dist = torch.cuda.FloatTensor(data_x_len, k+1)\n",
    "    else:\n",
    "        dist_mx = torch.LongTensor(data_x_len, k+1)\n",
    "        act_dist = torch.cuda.FloatTensor(data_x_len, k+1)\n",
    "    data_normalized = True if opt is not None and opt.normalize_data else False\n",
    "    largest = True if largest else (True if data_normalized else False)\n",
    "    \n",
    "    #compute l2 dist <--be memory efficient by blocking\n",
    "    total_chunks = int((data_x_len-1) // chunk_sz) + 1\n",
    "    y_t = data_y.t()\n",
    "    if not data_normalized:\n",
    "        y_norm = (data_y**2).sum(-1).view(1, -1)\n",
    "    \n",
    "    for i in range(total_chunks):\n",
    "        base = i*chunk_sz\n",
    "        upto = min((i+1)*chunk_sz, data_x_len)\n",
    "        cur_len = upto-base\n",
    "        x = data_x[base : upto]\n",
    "        \n",
    "        if not data_normalized:\n",
    "            x_norm = (x**2).sum(-1).view(-1, 1)        \n",
    "            #plus op broadcasts\n",
    "            dist = x_norm + y_norm        \n",
    "            dist -= 2*torch.mm(x, y_t)\n",
    "        else:\n",
    "            dist = -torch.mm(x, y_t)\n",
    "            \n",
    "        topk_d, topk = torch.topk(dist, k=k+1, dim=1, largest=largest)\n",
    "                \n",
    "        dist_mx[base:upto, :k+1] = topk #torch.topk(dist, k=k+1, dim=1, largest=largest)[1][:, 1:]\n",
    "        act_dist[base:upto, :k+1] = topk_d #torch.topk(dist, k=k+1, dim=1, largest=largest)[1][:, 1:]\n",
    "        \n",
    "    topk = dist_mx\n",
    "    if k > 3 and opt is not None and opt.sift:\n",
    "        #topk = dist_mx\n",
    "        #sift contains duplicate points, don't run this in general.\n",
    "        identity_ranks = torch.LongTensor(range(len(topk))).to(topk.device)\n",
    "        topk_0 = topk[:, 0]\n",
    "        topk_1 = topk[:, 1]\n",
    "        topk_2 = topk[:, 2]\n",
    "        topk_3 = topk[:, 3]\n",
    "\n",
    "        id_idx1 = topk_1 == identity_ranks\n",
    "        id_idx2 = topk_2 == identity_ranks\n",
    "        id_idx3 = topk_3 == identity_ranks\n",
    "\n",
    "        if torch.sum(id_idx1).item() > 0:\n",
    "            topk[id_idx1, 1] = topk_0[id_idx1]\n",
    "\n",
    "        if torch.sum(id_idx2).item() > 0:\n",
    "            topk[id_idx2, 2] = topk_0[id_idx2]\n",
    "\n",
    "        if torch.sum(id_idx3).item() > 0:\n",
    "            topk[id_idx3, 3] = topk_0[id_idx3]           \n",
    "\n",
    "    \n",
    "    if not include_self:\n",
    "        topk = topk[:, 1:]\n",
    "        act_dist = act_dist[:, 1:]\n",
    "    elif topk.size(-1) > k0:\n",
    "        topk = topk[:, :-1]\n",
    "    topk = topk.to(device_o)\n",
    "    return act_dist, topk\n",
    "\n",
    "class tokenizer:\n",
    "    \"\"\"\n",
    "    Rudimentary tokenizer for when allennlp is unavailable.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        import re\n",
    "        self.patt = re.compile('[ ;,.?!`\\'\":|\\s~%&*()#$@+-=]')\n",
    "        \n",
    "    def batch_tokenize(self, sent_l):\n",
    "        sent_l2 = []\n",
    "        for sent in sent_l:            \n",
    "            sent_l2.append(self.patt.split(sent))\n",
    "            \n",
    "        return sent_l2\n",
    "    \n",
    "class stop_word_filter:\n",
    "    \n",
    "    def filter_words(self, tok_l):\n",
    "        \"\"\"\n",
    "        Input: tok_l: list of tokens\n",
    "        \"\"\"\n",
    "        tok_l2 = []\n",
    "        for tok in tok_l:\n",
    "            if tok not in STOP_WORDS:\n",
    "                tok_l2.append(tok)\n",
    "        return tok_l2\n",
    "\n",
    "## This below is due to the authors of spacy, reproduced here as some users have ##\n",
    "## reported difficulties installing the language packages required for processig text ##\n",
    "\n",
    "# Stop words\n",
    "STOP_WORDS = set(\n",
    "    \"\"\"\n",
    "a about above across after afterwards again against all almost alone along\n",
    "already also although always am among amongst amount an and another any anyhow\n",
    "anyone anything anyway anywhere are around as at\n",
    "back be became because become becomes becoming been before beforehand behind\n",
    "being below beside besides between beyond both bottom but by\n",
    "call can cannot ca could\n",
    "did do does doing done down due during\n",
    "each eight either eleven else elsewhere empty enough even ever every\n",
    "everyone everything everywhere except\n",
    "few fifteen fifty first five for former formerly forty four from front full\n",
    "further\n",
    "get give go\n",
    "had has have he hence her here hereafter hereby herein hereupon hers herself\n",
    "him himself his how however hundred\n",
    "i if in indeed into is it its itself\n",
    "keep\n",
    "last latter latterly least less\n",
    "just\n",
    "made make many may me meanwhile might mine more moreover most mostly move much\n",
    "must my myself\n",
    "name namely neither never nevertheless next nine no nobody none noone nor not\n",
    "nothing now nowhere\n",
    "of off often on once one only onto or other others otherwise our ours ourselves\n",
    "out over own\n",
    "part per perhaps please put\n",
    "quite\n",
    "rather re really regarding\n",
    "same say see seem seemed seeming seems serious several she should show side\n",
    "since six sixty so some somehow someone something sometime sometimes somewhere\n",
    "still such\n",
    "take ten than that the their them themselves then thence there thereafter\n",
    "thereby therefore therein thereupon these they third this those though three\n",
    "through throughout thru thus to together too top toward towards twelve twenty\n",
    "two\n",
    "under until up unless upon us used using\n",
    "various very very via was we well were what whatever when whence whenever where\n",
    "whereafter whereas whereby wherein whereupon wherever whether which while\n",
    "whither who whoever whole whom whose why will with within without would\n",
    "yet you your yours yourself yourselves\n",
    "\"\"\".split()\n",
    ")\n",
    "\n",
    "contractions = [\"n't\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\"]\n",
    "STOP_WORDS.update(contractions)\n",
    "\n",
    "for apostrophe in [\"â€˜\", \"â€™\"]:\n",
    "    for stopword in contractions:\n",
    "        STOP_WORDS.add(stopword.replace(\"'\", apostrophe))\n",
    "\n",
    "def inner_mx(mx1, mx2):    \n",
    "    return torch.mm(mx1 * mx2.t())\n",
    "\n",
    "\n",
    "'''\n",
    "Input: lines is list of objects, not newline-terminated yet. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c8e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUE core (copied verbatim from mean.py)\n",
    "def compute_m(X, lamb, noise_vecs=None):\n",
    "    \n",
    "    X_cov = (lamb*cov(X))\n",
    "    #torch svd has bug. U and V not equal up to sign or permutation, for non-duplicate entries.\n",
    "    #U, D, Vt = (lamb*X_cov).svd()\n",
    "    \n",
    "    U, D, Vt = linalg.svd(X_cov.cpu().numpy())\n",
    "    U = torch.from_numpy(U.astype('float64')).to(device)\n",
    "    #torch can't take exponential on int64 types.\n",
    "    D_exp = torch.from_numpy(np.exp(D.astype('float64'))).to(device).diag()\n",
    "    \n",
    "    #projection of noise onto the singular vecs. \n",
    "    if noise_vecs is not None:\n",
    "        n_noise = noise_vecs.size(0)\n",
    "        print(utils.inner_mx(noise_vecs, U)[:, :int(1.5*n_noise)])\n",
    "                    \n",
    "    m = torch.mm(U, D_exp)\n",
    "    m = torch.mm(m, U.t())\n",
    "    \n",
    "    assert m.max().item() < float('Inf')    \n",
    "    m_tr =  m.diag().sum()\n",
    "    m = m / m_tr\n",
    "    \n",
    "    return m.to(torch.float32)\n",
    "\n",
    "\n",
    "def top_dir(X, opt, noise_vecs=None):\n",
    "    X = X - X.mean(dim=0, keepdim=True)    \n",
    "    X_cov = cov(X)\n",
    "    if False:\n",
    "        u, d, v_t = linalg.svd(X_cov.cpu().numpy())\n",
    "        #pdb.set_trace()\n",
    "        u = u[:opt.n_top_dir]        \n",
    "    else:\n",
    "        #convert to numpy tensor. \n",
    "        sv = decom.TruncatedSVD(opt.n_top_dir)\n",
    "        sv.fit(X.cpu().numpy())\n",
    "        u = sv.components_\n",
    "    \n",
    "    if noise_vecs is not None:\n",
    "        \n",
    "        print('inner of noise with top cov dirs')\n",
    "        n_noise = noise_vecs.size(0)\n",
    "        sv1 = decom.TruncatedSVD(n_noise)\n",
    "        sv1.fit(X.cpu().numpy())\n",
    "        u1 = torch.from_numpy(sv1.components_).to(device)\n",
    "        print(utils.inner_mx(noise_vecs, u1)[:, :int(1.5*n_noise)])\n",
    "    \n",
    "    #U, D, V = svd(X, k=1)    \n",
    "    return torch.from_numpy(u).to(device)\n",
    "    \n",
    "'''\n",
    "Input:\n",
    "-X: shape (n_sample, n_feat)\n",
    "'''\n",
    "\n",
    "def compute_tau1(X, select_idx, opt, noise_vecs):\n",
    "    \n",
    "    X = torch.index_select(X, dim=0, index=select_idx)\n",
    "    #input should already be centered!\n",
    "    X_centered = X - X.mean(0, keepdim=True)  \n",
    "    M = compute_m(X, opt.lamb, noise_vecs) \n",
    "    X_m = torch.mm(X_centered, M) #M should be symmetric, so not M.t()\n",
    "    tau1 = (X_centered*X_m).sum(-1)\n",
    "        \n",
    "    return tau1\n",
    "\n",
    "'''\n",
    "Input: already centered\n",
    "'''\n",
    "\n",
    "def compute_tau0(X, select_idx, opt, noise_vecs=None):\n",
    "    X = torch.index_select(X, dim=0, index=select_idx)\n",
    "    cov_dir = top_dir(X, opt, noise_vecs)\n",
    "    #top dir can be > 1\n",
    "    cov_dir = cov_dir.sum(dim=0, keepdim=True)\n",
    "    tau0 = (torch.mm(cov_dir, X.t())**2).squeeze()    \n",
    "    return tau0\n",
    "\n",
    "'''\n",
    "compute tau2, v^tM^{-1}v\n",
    "'''\n",
    "\n",
    "def compute_tau1_tau0(X, opt):\n",
    "    use_dom_eval = True\n",
    "    if use_dom_eval:\n",
    "        #dynamically set lamb now\n",
    "        #find dominant eval.\n",
    "        dom_eval, _ = utils.dominant_eval_cov(X)\n",
    "        opt.lamb = 1./dom_eval * opt.lamb_multiplier        \n",
    "        lamb = opt.lamb        \n",
    "\n",
    "    #noise_vecs can be used for visualization.\n",
    "    no_evec = True\n",
    "    if no_evec:\n",
    "        noise_vecs = None\n",
    "        \n",
    "    def get_select_idx(tau_method):\n",
    "        if device == 'cuda':\n",
    "            select_idx = torch.cuda.LongTensor(list(range(X.size(0))))\n",
    "        else:\n",
    "            select_idx = torch.LongTensor(list(range(X.size(0))))\n",
    "        n_removed = 0\n",
    "        for _ in range(opt.n_iter):\n",
    "            tau1 = tau_method(X, select_idx, opt, noise_vecs)\n",
    "            #select idx to keep\n",
    "            cur_select_idx = torch.topk(tau1, k=int(tau1.size(0)*(1-opt.remove_p)), largest=False)[1]\n",
    "            #note these points are indices of current iteration            \n",
    "            n_removed += (select_idx.size(0) - cur_select_idx.size(0))\n",
    "            select_idx = torch.index_select(select_idx, index=cur_select_idx, dim=0)            \n",
    "        return select_idx, n_removed, tau1\n",
    "\n",
    "    if opt.fast_jl:\n",
    "        select_idx1, n_removed1, tau1 = get_select_idx(compute_tau1_fast)\n",
    "    else:\n",
    "        select_idx1, n_removed1, tau1 = get_select_idx(compute_tau1)\n",
    "        \n",
    "    #acc1 = compute_acc_with_idx(select_idx, cor_idx, X, n_removed)    \n",
    "    if DEBUG:\n",
    "        print('new acc1 {}'.format(acc1))\n",
    "        M = compute_m(X, opt.lamb, noise_vecs)\n",
    "        X_centered = X - X.mean(0,keepdim=True)\n",
    "        X_m = torch.mm(X_centered, M) #M should be symmetric, so not M.t()\n",
    "        tau1 = (X_centered*X_m).sum(-1)\n",
    "        print('old acc1 {}'.format(compute_acc(tau1, cor_idx)))\n",
    "        pdb.set_trace()\n",
    "    \n",
    "    '''\n",
    "    if device == 'cuda':\n",
    "        select_idx = torch.cuda.LongTensor(range(X.size(0)))\n",
    "    else:\n",
    "        select_idx = torch.LongTensor(range(X.size(0)))\n",
    "    for _ in range(opt.n_iter):\n",
    "        tau0 = compute_tau0(X, select_idx, opt)\n",
    "        cur_select_idx = torch.topk(tau0, k=tau1.size(0)*(1-opt.remove_p), largest=False)[1]\n",
    "        select_idx = torch.index_select(select_idx, index=cur_select_idx, dim=0)\n",
    "    '''\n",
    "    select_idx0, n_removed0, tau0 = get_select_idx(compute_tau0)    \n",
    "    \n",
    "    return tau1, select_idx1, n_removed1, tau0, select_idx0, n_removed0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c408025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline methods (copied verbatim from baselines.py)\n",
    "def l2(X):\n",
    "    scores = ((X - X.mean(0))**2).sum(-1)    \n",
    "    return scores\n",
    "    \n",
    "'''\n",
    "Input:\n",
    "-X, Y: 2D tensors\n",
    "'''\n",
    "\n",
    "def isolation_forest(X):\n",
    "    X = X.cpu().numpy()\n",
    "    model = sklearn.ensemble.IsolationForest(contamination='auto', behaviour='new')\n",
    "    #labels = model.fit_predict(X)\n",
    "    model.fit(X)\n",
    "    scores = -model.decision_function(X)\n",
    "    \n",
    "    #labels = torch.from_numpy(labels).to(utils.device)  \n",
    "    #scores = torch.zeros_like(labels)\n",
    "    #scores[labels==-1] = 1\n",
    "    return torch.from_numpy(scores).to(utils.device)\n",
    "\n",
    "'''\n",
    "Elliptic envelope\n",
    "Returns: The higher the score, the more likely to be outlier.\n",
    "'''\n",
    "\n",
    "def knn_dist_lof(X, k=10):\n",
    "    X_len = len(X)\n",
    "    \n",
    "    #dist_ = dist(X, X)    \n",
    "    #min_dist, min_idx = torch.topk(dist_, dim=-1, k=k, largest=False)\n",
    "    \n",
    "    min_dist, min_idx = utils.dist_rank(X, k=k, largest=False)\n",
    "    kth_dist = min_dist[:, -1]\n",
    "    # sum max(kth dist, dist(o, p)) over neighbors o of p\n",
    "    kth_dist_exp = kth_dist.expand(X.size(0), -1) #n x n\n",
    "    kth_dist = torch.gather(input=kth_dist_exp, dim=1, index=min_idx)\n",
    "    \n",
    "    min_dist[kth_dist > min_dist] = kth_dist[kth_dist > min_dist]\n",
    "    #inverse of lrd scores\n",
    "    dist_avg = min_dist.mean(-1).clamp(min=0.0001)\n",
    "    \n",
    "    compare_density = False\n",
    "    if compare_density:\n",
    "        #compare with density. Get kth neighbor index.\n",
    "        dist_avg_exp = dist_avg.unsqueeze(-1) / dist_avg.unsqueeze(0).expand(X_len, -1)\n",
    "        #lof = torch.zeros(X_len, 1).to(utils.device)\n",
    "        lof = torch.gather(input=dist_avg_exp, dim=-1, index=min_idx).sum(-1)\n",
    "        torch.scatter_add_(lof, dim=-1, index=min_idx, src=dist_avg_exp)    \n",
    "        return -lof.squeeze(0)\n",
    "\n",
    "    return dist_avg\n",
    "\n",
    "'''\n",
    "LoOP: kNN based method using quadratic mean distance to estimate density.\n",
    "LoOP (Local Outlier Probabilities) (Kriegel et al. 2009a)\n",
    "'''\n",
    "\n",
    "def ellenv(X):\n",
    "    X = X.cpu().numpy()\n",
    "    model = sklearn.covariance.EllipticEnvelope(contamination=0.2)\n",
    "    #ensemble.IsolationForest(contamination='auto', behaviour='new')\n",
    "    model.fit(X)\n",
    "    scores = -model.decision_function(X)\n",
    "    \n",
    "    #labels = torch.from_numpy(labels).to(utils.device)  \n",
    "    #scores = torch.zeros_like(labels)\n",
    "    #scores[labels==-1] = 1\n",
    "    return torch.from_numpy(scores).to(utils.device)\n",
    "\n",
    "'''\n",
    "Local outlier factor.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0757b1f0",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "Define your tabular data matrix `X` as a `torch.FloatTensor` on `utils.device`.\n",
    "\n",
    "Example (you provide X):\n",
    "```python\n",
    "# X = your torch.FloatTensor of shape (n_samples, n_features)\n",
    "# X = X - X.mean(0)  # optional centering\n",
    "```\n",
    "Then obtain QUE and spectral scores:\n",
    "```python\n",
    "opt = utils.parse_args()  # default args\n",
    "opt.remove_p = 0.1\n",
    "opt.lamb_multiplier = 4\n",
    "opt.n_top_dir = 1\n",
    "opt.n_iter = 1\n",
    "opt.fast_jl = False\n",
    "tau1, select_idx1, n_removed1, tau0, select_idx0, n_removed0 = compute_tau1_tau0(X, opt)\n",
    "```\n",
    "And run baselines (examples):\n",
    "```python\n",
    "tau_l2 = l2(X)\n",
    "tau_if = isolation_forest(X)\n",
    "tau_lof = knn_dist_lof(X)\n",
    "tau_ell = ellenv(X)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
