{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qi6hSrQvB6U"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Imports and basic setup\n",
        "# =========================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "import iisignature\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, roc_auc_score\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "import sigkernel\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pylab as pylab\n",
        "\n",
        "params = {\n",
        "    'legend.fontsize': 'large',\n",
        "    'figure.figsize': (16, 5),\n",
        "    'axes.labelsize': 'large',\n",
        "    'axes.titlesize': 'large',\n",
        "    'xtick.labelsize': 'large',\n",
        "    'ytick.labelsize': 'large'\n",
        "}\n",
        "pylab.rcParams.update(params)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Error function\n",
        "# =========================\n",
        "# In the Bitcoin notebook this was a true MAPE, but that breaks for 0 labels.\n",
        "# Here we just use plain MAE on 0/1 labels, but keep the function name\n",
        "# so the rest of the code changes minimally.\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    For our binary 0/1 labels, this is actually mean absolute error.\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true).reshape(-1)\n",
        "    y_pred = np.asarray(y_pred).reshape(-1)\n",
        "    return np.mean(np.abs(y_true - y_pred))\n"
      ],
      "metadata": {
        "id": "f_nZGQoLvQJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Helper: windowing for multivariate account data\n",
        "# =========================\n",
        "# X_account: numpy array of shape (T, M) with binary 0/1 interest per CUSIP.\n",
        "# target_cusip_idx: integer index of the CUSIP we want to predict.\n",
        "# h_window: length of the historical window (in time steps).\n",
        "\n",
        "def build_account_windows(X_account, target_cusip_idx, h_window=36):\n",
        "    \"\"\"\n",
        "    Build sliding windows for a single account.\n",
        "\n",
        "    Input:\n",
        "        X_account: array of shape (T, M)\n",
        "        target_cusip_idx: int, column index of target CUSIP\n",
        "        h_window: int, number of past steps in each input window\n",
        "\n",
        "    Returns:\n",
        "        X_windows: array of shape (N, h_window, M)\n",
        "        y: array of shape (N,) with 0/1 labels for target CUSIP at t+1\n",
        "    \"\"\"\n",
        "    T, M = X_account.shape\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "\n",
        "    # We need at least h_window + 1 steps to have one input and next-step label\n",
        "    for t in range(h_window, T - 1):\n",
        "        # window of past h_window steps\n",
        "        X_list.append(X_account[t - h_window:t, :])\n",
        "        # label: interest in target CUSIP at t+1 (next time step)\n",
        "        y_list.append(X_account[t + 1, target_cusip_idx])\n",
        "\n",
        "    X_windows = np.array(X_list, dtype=float)     # (N, h_window, M)\n",
        "    y = np.array(y_list, dtype=float).reshape(-1) # (N,)\n",
        "\n",
        "    return X_windows, y\n"
      ],
      "metadata": {
        "id": "pDxb0ZvivQNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# (Optional) Plot helper\n",
        "# =========================\n",
        "# If you ever want to visualise predicted probabilities vs true 0/1 labels.\n",
        "\n",
        "def PlotResult(y_train, y_test, y_train_predict, y_test_predict, name):\n",
        "\n",
        "    train_len = len(y_train)\n",
        "    test_len = len(y_test)\n",
        "\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 5))\n",
        "\n",
        "    ax.plot(y_train_predict, color='red', label='Train predicted p(interest)')\n",
        "    ax.plot(range(train_len, train_len + test_len),\n",
        "            y_test_predict,\n",
        "            color='red', linestyle='--',\n",
        "            label='Test predicted p(interest)')\n",
        "\n",
        "    ax.plot(np.concatenate([y_train, y_test]),\n",
        "            color='green',\n",
        "            label='Actual interest (0/1)')\n",
        "\n",
        "    ax.axvspan(len(y_train), len(y_train) + len(y_test),\n",
        "               alpha=0.3, color='lightgrey')\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.axis('tight')\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.xlabel('Time index (windowed)')\n",
        "    plt.ylabel('Interest in target CUSIP (probability / 0-1)')\n",
        "    plt.title(f'Account propensity prediction ({name})')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "DLJXAHz2vQRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Load / prepare account data\n",
        "# =========================\n",
        "# ---- YOU need to provide `client_cusip_binary` and choose account + target CUSIP ----\n",
        "#\n",
        "# We assume you already created a dictionary elsewhere like:\n",
        "#   client_cusip_binary[account_id] = DataFrame\n",
        "#       index  = timestamps (sorted)\n",
        "#       columns = CUSIP IDs (strings or ints)\n",
        "#       values = 0/1 binary interaction indicators\n",
        "#\n",
        "# Here we just plug that in.\n",
        "\n",
        "# Example placeholders (replace these with actual objects in your environment):\n",
        "\n",
        "# client_cusip_binary = pickle.load(open('client_cusip_binary.pkl', 'rb'))\n",
        "# ACCOUNT_ID = 'some_account_id'\n",
        "# TARGET_CUSIP_ID = 'some_cusip'\n",
        "\n",
        "# ---- START of the user-specific block ----\n",
        "ACCOUNT_ID = 'REPLACE_ME_ACCOUNT'\n",
        "TARGET_CUSIP_ID = 'REPLACE_ME_CUSIP'\n",
        "\n",
        "# This must exist in your environment:\n",
        "# client_cusip_binary = {account_id: df_account, ...}\n",
        "# Each df_account is a DataFrame index=dates, columns=cusips, values={0,1}\n",
        "df_account = client_cusip_binary[ACCOUNT_ID].copy()\n",
        "\n",
        "# Sort by time just to be safe\n",
        "df_account = df_account.sort_index()\n",
        "\n",
        "# Make sure target CUSIP is present\n",
        "assert TARGET_CUSIP_ID in df_account.columns, \"Target CUSIP not in account's CUSIP list.\"\n",
        "\n",
        "cusip_list = df_account.columns.tolist()\n",
        "target_cusip_idx = cusip_list.index(TARGET_CUSIP_ID)\n",
        "\n",
        "# Convert to numpy\n",
        "X_account = df_account.values.astype(float)  # shape (T, M)\n",
        "T, M = X_account.shape\n",
        "print(f\"Account {ACCOUNT_ID}: T={T}, M={M} (CUSIPs)\")\n"
      ],
      "metadata": {
        "id": "C_hgetvTvQUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Create windows and labels for this account + target CUSIP\n",
        "# =========================\n",
        "\n",
        "h_window = 36  # same order as in Bitcoin example; tune later if needed\n",
        "\n",
        "X_window, y = build_account_windows(X_account, target_cusip_idx, h_window=h_window)\n",
        "\n",
        "print(f\"Shapes: X_window = {X_window.shape}, y = {y.shape}\")\n",
        "# X_window: (N, h_window, M)\n",
        "# y: (N,)\n"
      ],
      "metadata": {
        "id": "XC2BfibDvQYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Transform paths for signature kernels\n",
        "# =========================\n",
        "# We follow the same pattern as the BTC notebook:\n",
        "#    - convert to torch tensor\n",
        "#    - use sigkernel.transform to add time and lead-lag if desired\n",
        "#      (this function can handle multivariate paths of shape (N, L, dim)).\n",
        "\n",
        "# convert to torch\n",
        "X_window_torch = torch.tensor(X_window, dtype=torch.float64)      # shape (N, L, M)\n",
        "\n",
        "# apply augmentation: time + lead-lag (same arguments as original notebook)\n",
        "X_window_torch = sigkernel.transform(X_window_torch, at=True, ll=True, scale=1e-5)\n",
        "\n",
        "# train / test split (no shuffle because it's time series)\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    X_window_torch.numpy(), y, test_size=0.2, shuffle=False\n",
        ")\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype=torch.float64, device='cpu')\n",
        "x_test = torch.tensor(x_test, dtype=torch.float64, device='cpu')\n",
        "\n",
        "y_train = np.array(y_train, dtype=float).reshape(-1)\n",
        "y_test = np.array(y_test, dtype=float).reshape(-1)\n",
        "\n",
        "print(\"After split:\")\n",
        "print(\"  x_train:\", x_train.shape)\n",
        "print(\"  x_test :\", x_test.shape)\n",
        "print(\"  y_train:\", y_train.shape)\n",
        "print(\"  y_test :\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "75McXEdcvdxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Training phase: hyperparameters for SVR\n",
        "# =========================\n",
        "\n",
        "svr_parameters = {\n",
        "    'C': np.logspace(0, 4, 5),\n",
        "    'gamma': np.logspace(-4, 4, 9)\n",
        "}\n"
      ],
      "metadata": {
        "id": "ZjNajYdrvd1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Training: truncated signature features + SVR\n",
        "# =========================\n",
        "\n",
        "best_error_sig = 1e8\n",
        "\n",
        "# Depth, scale, kernel, normalization search as before\n",
        "for depth in tqdm([2, 3, 4, 5, 6], desc=\"Signature depth\"):\n",
        "    for scale in tqdm([1., 2., 3., 4., 5.], leave=False, desc=\"Scale\"):\n",
        "        for ker in tqdm(['linear', 'rbf'], leave=False, desc=\"SVR kernel\"):\n",
        "            for normalize in tqdm([True, False], leave=False, desc=\"Normalize?\"):\n",
        "\n",
        "                # Truncated signatures on the transformed paths\n",
        "                # x_train: shape (N_train, L_aug, dim_aug)\n",
        "                sig_train = iisignature.sig(scale * x_train.numpy(), depth)\n",
        "\n",
        "                # normalization\n",
        "                if normalize:\n",
        "                    sig_train = sigkernel.normalize(\n",
        "                        sig_train,\n",
        "                        x_train.shape[-1],  # dim of path\n",
        "                        depth\n",
        "                    )\n",
        "\n",
        "                # fit the model\n",
        "                svr = SVR(kernel=ker)\n",
        "                svr_sig = GridSearchCV(\n",
        "                    estimator=svr,\n",
        "                    param_grid=svr_parameters,\n",
        "                    cv=5,\n",
        "                    n_jobs=-1\n",
        "                )\n",
        "                svr_sig.fit(sig_train, y_train)\n",
        "\n",
        "                # select best model (criterion: R^2 close to 1)\n",
        "                if np.abs(1. - svr_sig.best_score_) < np.abs(1. - best_error_sig):\n",
        "                    best_sig_model = svr_sig\n",
        "                    best_error_sig = svr_sig.best_score_\n",
        "                    best_depth_sig = depth\n",
        "                    best_scale_sig = scale\n",
        "                    best_ker_sig = ker\n",
        "                    normalize_sig = normalize\n",
        "\n",
        "print(\"Best truncated-signature model:\")\n",
        "print(f\"  depth={best_depth_sig}, scale={best_scale_sig}, kernel={best_ker_sig}, normalize={normalize_sig}\")\n",
        "print(f\"  CV R^2 = {best_error_sig:.4f}\")\n"
      ],
      "metadata": {
        "id": "tWXsERm1vd5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Training: signature PDE kernel + SVR (precomputed kernel)\n",
        "# =========================\n",
        "\n",
        "best_error_pde = 1e8\n",
        "\n",
        "for sigma in tqdm([5e-2, 1e-1, 2.5e-1, 5e-1, 7.5e-1, 1.], desc=\"Sigma (RBF static kernel)\"):\n",
        "\n",
        "    # Specify the static kernel\n",
        "    static_kernel = sigkernel.RBFKernel(sigma=sigma)\n",
        "\n",
        "    # Initialize the corresponding signature kernel\n",
        "    signature_kernel = sigkernel.SigKernel(static_kernel, dyadic_order=0)\n",
        "\n",
        "    # Gram matrix train\n",
        "    G_train = signature_kernel.compute_Gram(x_train, x_train, sym=True).numpy()\n",
        "\n",
        "    # fit the model\n",
        "    svr = SVR(kernel='precomputed')\n",
        "    svr_pde = GridSearchCV(\n",
        "        estimator=svr,\n",
        "        param_grid=svr_parameters,\n",
        "        cv=5,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    svr_pde.fit(G_train, y_train)\n",
        "\n",
        "    if np.abs(1. - svr_pde.best_score_) < np.abs(1. - best_error_pde):\n",
        "        best_pde_model = svr_pde\n",
        "        best_error_pde = svr_pde.best_score_\n",
        "        best_sigma = sigma\n",
        "\n",
        "print(\"Best PDE-signature model:\")\n",
        "print(f\"  sigma={best_sigma}, CV R^2 = {best_error_pde:.4f}\")\n"
      ],
      "metadata": {
        "id": "SFU_fL32vd90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Testing phase\n",
        "# =========================\n",
        "\n",
        "kernels = ['rbf', 'sig', 'sig_PDE']  # I dropped 'gak' because we never defined it in this notebook\n",
        "\n",
        "final = {}\n"
      ],
      "metadata": {
        "id": "0krwoPQFveBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Evaluation on test set\n",
        "# =========================\n",
        "\n",
        "for ker in tqdm(kernels):\n",
        "\n",
        "    if ker == 'sig_PDE':\n",
        "\n",
        "        # Rebuild signature kernel with the best sigma\n",
        "        static_kernel = sigkernel.RBFKernel(sigma=best_sigma)\n",
        "        signature_kernel = sigkernel.SigKernel(static_kernel, dyadic_order=0)\n",
        "\n",
        "        # Gram matrices\n",
        "        G_train = signature_kernel.compute_Gram(x_train, x_train, sym=True).numpy()\n",
        "        G_test = signature_kernel.compute_Gram(x_test, x_train, sym=False).numpy()\n",
        "\n",
        "        # predict\n",
        "        y_train_predict = best_pde_model.predict(G_train)\n",
        "        y_test_predict = best_pde_model.predict(G_test)\n",
        "\n",
        "        # Clip to [0,1] to interpret as probabilities if you wish\n",
        "        y_train_predict_clipped = np.clip(y_train_predict, 0.0, 1.0)\n",
        "        y_test_predict_clipped = np.clip(y_test_predict, 0.0, 1.0)\n",
        "\n",
        "        # calculate errors (MAE-style) and AUC\n",
        "        p_error_test = mean_absolute_percentage_error(y_test, y_test_predict_clipped)\n",
        "        try:\n",
        "            auc_test = roc_auc_score(y_test, y_test_predict_clipped)\n",
        "        except ValueError:\n",
        "            auc_test = np.nan  # if only one class appears in test\n",
        "\n",
        "        final[(ker, 'MAE')] = p_error_test\n",
        "        final[(ker, 'AUC')] = auc_test\n",
        "\n",
        "    elif ker == 'sig':\n",
        "\n",
        "        # truncated signatures for train and test\n",
        "        sig_train = iisignature.sig(best_scale_sig * x_train.numpy(), best_depth_sig)\n",
        "        sig_test = iisignature.sig(best_scale_sig * x_test.numpy(), best_depth_sig)\n",
        "\n",
        "        # normalization\n",
        "        if normalize_sig:\n",
        "            sig_train = sigkernel.normalize(sig_train, x_train.shape[-1], best_depth_sig)\n",
        "            sig_test = sigkernel.normalize(sig_test, x_test.shape[-1], best_depth_sig)\n",
        "\n",
        "        # predict\n",
        "        y_train_predict = best_sig_model.predict(sig_train)\n",
        "        y_test_predict = best_sig_model.predict(sig_test)\n",
        "\n",
        "        y_train_predict_clipped = np.clip(y_train_predict, 0.0, 1.0)\n",
        "        y_test_predict_clipped = np.clip(y_test_predict, 0.0, 1.0)\n",
        "\n",
        "        # calculate errors and AUC\n",
        "        p_error_test = mean_absolute_percentage_error(y_test, y_test_predict_clipped)\n",
        "        try:\n",
        "            auc_test = roc_auc_score(y_test, y_test_predict_clipped)\n",
        "        except ValueError:\n",
        "            auc_test = np.nan\n",
        "\n",
        "        final[(ker, f'depth_{best_depth_sig}', f'ker_{best_ker_sig}', 'MAE')] = p_error_test\n",
        "        final[(ker, f'depth_{best_depth_sig}', f'ker_{best_ker_sig}', 'AUC')] = auc_test\n",
        "\n",
        "    elif ker == 'rbf':\n",
        "        # Optionally: a simple baseline SVR on raw (flattened) windows or something else.\n",
        "        # For minimal changes, we can skip or implement a very simple baseline.\n",
        "\n",
        "        # Example: flatten the augmented path and fit a vanilla RBF SVR\n",
        "        X_flat_train = x_train.numpy().reshape(x_train.shape[0], -1)\n",
        "        X_flat_test = x_test.numpy().reshape(x_test.shape[0], -1)\n",
        "\n",
        "        svr_rbf = GridSearchCV(\n",
        "            estimator=SVR(kernel='rbf'),\n",
        "            param_grid=svr_parameters,\n",
        "            cv=5,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        svr_rbf.fit(X_flat_train, y_train)\n",
        "\n",
        "        y_test_predict = svr_rbf.predict(X_flat_test)\n",
        "        y_test_predict_clipped = np.clip(y_test_predict, 0.0, 1.0)\n",
        "\n",
        "        p_error_test = mean_absolute_percentage_error(y_test, y_test_predict_clipped)\n",
        "        try:\n",
        "            auc_test = roc_auc_score(y_test, y_test_predict_clipped)\n",
        "        except ValueError:\n",
        "            auc_test = np.nan\n",
        "\n",
        "        final[(ker, 'MAE')] = p_error_test\n",
        "        final[(ker, 'AUC')] = auc_test\n",
        "\n",
        "# Save results\n",
        "with open('../results/account_propensity_results.pkl', 'wb') as file:\n",
        "    pickle.dump(final, file)\n"
      ],
      "metadata": {
        "id": "S9sxAePFveFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Print results\n",
        "# =========================\n",
        "\n",
        "with open('../results/account_propensity_results.pkl', 'rb') as file:\n",
        "    final_loaded = pickle.load(file)\n",
        "\n",
        "print(\"Final test errors and AUCs (per kernel):\")\n",
        "for k, v in final_loaded.items():\n",
        "    print(k, \":\", v)\n"
      ],
      "metadata": {
        "id": "sJuajvoOvQcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wqhP_UTTv3OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ykW2fsu3v3cZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
