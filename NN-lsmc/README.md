# Deep Learning for American Put Option Pricing

Implementation of the paper "Pricing and hedging American-style options with deep learning" by Becker, Cheridito, and Jentzen (2019) for 1-dimensional American Put options.

## Overview

This implementation uses deep neural networks to:
1. **Learn optimal stopping strategies** via a neural network variant of the Longstaff-Schwartz algorithm
2. **Compute price bounds** using lower and upper bound estimates
3. **Calculate confidence intervals** for the option price
4. **Construct hedging strategies** (in full version)

## Key Components

### 1. Continuation Value Networks
- Architecture: 2 hidden layers with 50 nodes each, tanh activation
- Input: Augmented state (stock price S, discounted payoff)
- Output: Continuation value (expected value if not exercising)
- Training: Backward recursion from maturity using MSE loss

### 2. Optimal Stopping Strategy
The learned strategy exercises when:
```
Immediate Payoff ≥ Continuation Value
max(K - S, 0) ≥ c_θ(S, payoff)
```

### 3. Lower Bound
- Generated by applying the learned stopping rule to fresh simulations
- Biased low (always valid exercise strategy, may not be optimal)

### 4. Upper Bound
- Uses dual martingale approach (Rogers 2002)
- Requires nested simulation to compute martingale increments
- Biased high

### 5. Point Estimate & Confidence Interval
- Point estimate: (Lower + Upper) / 2
- 95% CI: [Lower - 1.96*SE_L, Upper + 1.96*SE_U]

## Files

- `american_put_pricing.py` - Full implementation with hedging
- `american_put_simple.py` - Simplified version for quick testing

## Usage

```python
from american_put_simple import AmericanPutPricer

# Create pricer for American Put
pricer = AmericanPutPricer(
    S0=100,      # Initial stock price
    K=100,       # Strike price
    r=0.05,      # Risk-free rate (5%)
    sigma=0.2,   # Volatility (20%)
    T=1.0,       # Maturity (1 year)
    N=9          # Number of exercise dates
)

# Train continuation value networks
pricer.train_continuation_values(K=50000, batch_size=4096)

# Compute price bounds
L, std_L = pricer.compute_lower_bound(K=100000)
U, std_U = pricer.compute_upper_bound(K_outer=512, K_inner=512)

# Point estimate
V = (L + U) / 2
```

## Algorithm Details

### Training Continuation Values (Section 2)

```
For n = N-1 down to 0:
    1. Create neural network c_θ(S, payoff) 
    2. If n < N-1, warm-start from c_θ_{n+1}
    3. Minimize: Σ[G_τ_{n+1} - c_θ(S_n, payoff_n)]²
       where τ_{n+1} is the stopping time from next iteration
    4. Update stopping rule: τ_n = n if payoff_n ≥ c_θ(S_n, payoff_n)
                                    else τ_{n+1}
```

### Computing Bounds (Section 3)

**Lower Bound:**
```
1. Simulate K_L fresh paths
2. Apply learned stopping rule τ_Θ
3. L = E[G_τ_Θ] ≈ (1/K_L) Σ g(τ_k, S_τ_k)
```

**Upper Bound (Nested Simulation):**
```
For each outer path k = 1,...,K_U:
    1. Compute martingale M via nested simulation
    2. For each time n:
        - Simulate K_inner paths from S_n
        - Apply learned strategy to get continuation value
        - M_{n+1} = M_n + (nested_estimate - payoff_n)
    3. Compute max_n[payoff_n - M_n]
4. U = E[max_n(G_n - M_n)]
```

## Implementation Notes

### Why Augmented State?
The paper found that using (S, discounted_payoff) as input works better than just S:
- Helps the network learn more efficiently
- Provides explicit payoff information
- Empirically improves convergence

### Warm Starting
Networks for time n are initialized from trained weights at time n+1:
- Speeds up training (3500 epochs vs 6000 for first network)
- Leverages similarity of continuation values across adjacent times
- Improves stability

### Batch Normalization
Applied after each linear layer (except output):
- Stabilizes training
- Allows higher learning rates
- Improves generalization

### Training Schedule
- First network (t = T - Δt): 4000-6000 epochs from scratch
- Subsequent networks: 2000-3500 epochs with warm start
- Adam optimizer with learning rate 1e-3

## Computational Complexity

| Operation | Complexity | Typical Time (GPU) |
|-----------|-----------|-------------------|
| Train continuation values | O(K × N × epochs) | ~30-60 seconds |
| Lower bound | O(K_L × N) | ~5 seconds |
| Upper bound | O(K_U × K_inner × N²) | ~5-10 minutes |

Upper bound is expensive due to nested simulation. Can reduce K_outer and K_inner for faster (but less accurate) estimates.

## Expected Results

For S0=100, K=100, r=5%, σ=20%, T=1, N=9:

```
Lower bound:     ~6.04
Upper bound:     ~6.08
Point estimate:  ~6.06
95% CI:          [6.02, 6.10]
Spread:          ~0.04
```

Analytical American Put price (via binomial tree): ~6.05

## Comparison to Paper

This implementation:
- ✅ Matches network architecture (2 hidden layers, tanh activation)
- ✅ Uses augmented state (S, payoff)
- ✅ Implements backward recursion for continuation values
- ✅ Computes lower/upper bounds with confidence intervals
- ✅ Simplified for 1D case (paper handles multi-dimensional)
- ⚠️  Reduced sample sizes for faster runtime
- ⚠️  Hedging strategy training included in full version only

## References

Becker, S., Cheridito, P., & Jentzen, A. (2019). Pricing and hedging American-style options with deep learning. arXiv:1912.11060

## Requirements

```
torch>=1.9.0
numpy>=1.19.0
scipy>=1.5.0
matplotlib>=3.3.0  # for visualization (optional)
```

## License

MIT License - See paper for academic citation requirements
