{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# American Put Option Pricing — LSM · NLSM · RLSM\n",
    "\n",
    "**Source repository:** [HeKrRuTe/OptStopRandNN](https://github.com/HeKrRuTe/OptStopRandNN)  \n",
    "**Paper:** *Optimal Stopping via Randomized Neural Networks*, Herrera, Krach, Ruyssen & Teichmann (2023), *Frontiers of Mathematical Finance*\n",
    "\n",
    "This notebook is a **self-contained port** of three algorithm modules from the repo:\n",
    "\n",
    "| Notebook section | Repo source file(s) | Notes |\n",
    "|---|---|---|\n",
    "| `Put1Dim` payoff | `payoff.py` | Verbatim |\n",
    "| `BlackScholes.generate_paths()` | `stock_model.py` | Verbatim; other models stripped |\n",
    "| `AmericanOptionPricer.price()` + `stop()` | `backward_induction_pricer.py` | Verbatim; Greeks/bounds stripped |\n",
    "| `BasisFunctions` (polynomial) | `algorithms/utils/basis_functions.py` | Not uploaded — inlined from usage in `regression.py` |\n",
    "| `LeastSquares.calculate_regression()` | `regression.py` | Verbatim |\n",
    "| `LeastSquaresPricer` | `LSM.py` | Verbatim |\n",
    "| `Reservoir2` (frozen random network) | `algorithms/utils/randomized_neural_networks.py` | Not uploaded — inlined from usage in `regression.py` |\n",
    "| `ReservoirLeastSquares2.calculate_regression()` | `regression.py` | Verbatim; already PyTorch |\n",
    "| `ReservoirLeastSquarePricerFastTanh` | `RLSM.py` | Verbatim |\n",
    "| `NetworkNLSM` architecture | `algorithms/utils/neural_networks.py` | Not uploaded; **converted TF/Keras → PyTorch** |\n",
    "| `NeuralRegression.train_network()` / `evaluate_network()` | `NLSM.py` | Verbatim — already used PyTorch |\n",
    "| `NeuralNetworkPricer` | `NLSM.py` | Verbatim |\n",
    "\n",
    "> **TensorFlow → PyTorch conversion:**  \n",
    "> The only TF dependency was `NetworkNLSM` in `neural_networks.py` (not uploaded), which was a `tf.keras.Sequential` model.  \n",
    "> It is rewritten here as a `torch.nn.Module` with the **identical architecture**: two hidden layers with `Tanh` activation.  \n",
    "> Everything else in `NLSM.py` (`NeuralRegression.train_network`, `evaluate_network`) already used PyTorch in the original.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 · Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch — used by RLSM's Reservoir2 and NLSM's NeuralRegression\n",
    "# (regression.py and NLSM.py both imported torch in the original)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as tdata\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(f'PyTorch  : {torch.__version__}')\n",
    "print(f'NumPy    : {np.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 · Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option & model parameters\n",
    "SPOT       = 100.0\n",
    "STRIKE     = 100.0\n",
    "RATE       = 0.05\n",
    "VOLATILITY = 0.2\n",
    "MATURITY   = 1.0\n",
    "DIVIDEND   = 0.0\n",
    "\n",
    "# Simulation parameters\n",
    "NB_PATHS   = 50_000\n",
    "NB_DATES   = 50\n",
    "\n",
    "print(f'S0={SPOT}  K={STRIKE}  r={RATE}  σ={VOLATILITY}  T={MATURITY}  q={DIVIDEND}')\n",
    "print(f'Paths={NB_PATHS:,}   Dates={NB_DATES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 · Payoff — `Put1Dim`\n",
    "\n",
    "**Source:** `payoff.py` → `Put1Dim`  \n",
    "Ported verbatim. Boilerplate base class and unused payoffs omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: payoff.py → Put1Dim (verbatim; base class boilerplate omitted)\n",
    "class Put1Dim:\n",
    "    \"\"\"Payoff of a 1-dimensional put option: max(K - S, 0).\n",
    "    Source: payoff.py::Put1Dim\n",
    "    \"\"\"\n",
    "    def __init__(self, strike):\n",
    "        self.strike = strike\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.eval(X)\n",
    "\n",
    "    def eval(self, X):\n",
    "        # X is (nb_paths, nb_stocks) when called on a single date slice,\n",
    "        # or (nb_paths, nb_stocks, nb_dates+1) when called on full paths.\n",
    "        # The pricer calls payoff(stock_paths) on full paths to get all payoffs,\n",
    "        # then payoff.eval(stock_paths[:,:,date]) for per-date slices.\n",
    "        if X.ndim == 3:\n",
    "            return np.maximum(0, self.strike - X[:, 0, :])  # (nb_paths, nb_dates+1)\n",
    "        return np.maximum(0, self.strike - X[:, 0])          # (nb_paths,)\n",
    "\n",
    "payoff = Put1Dim(strike=STRIKE)\n",
    "print(f'Payoff at S=90 : {payoff.eval(np.array([[[90]]]))[0,0]:.2f}  (expected 10)')\n",
    "print(f'Payoff at S=110: {payoff.eval(np.array([[[110]]]))[0,0]:.2f}  (expected  0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 · Stock Model — `BlackScholes`\n",
    "\n",
    "**Source:** `stock_model.py` → `Model` (base) + `BlackScholes`  \n",
    "`generate_paths()` copied **verbatim** including the exact vectorised `cumsum/exp` formula.  \n",
    "Removed: FBM, Heston, RoughHeston, joblib parallelism, draw utilities.  \n",
    "Array layout preserved: `(nb_paths, nb_stocks, nb_dates+1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: stock_model.py → Model (base attrs only) + BlackScholes.generate_paths() (verbatim)\n",
    "class BlackScholes:\n",
    "    \"\"\"GBM under risk-neutral measure.\n",
    "    Source: stock_model.py::BlackScholes\n",
    "    Removed: all other model classes, joblib, fbm dependency.\n",
    "    \"\"\"\n",
    "    def __init__(self, drift, volatility, nb_paths, nb_stocks, nb_dates,\n",
    "                 spot, maturity, dividend=0.0, **kwargs):\n",
    "        # From stock_model.py::Model.__init__\n",
    "        self.name       = 'BlackScholes'\n",
    "        self.drift      = drift - dividend   # risk-neutral drift\n",
    "        self.rate       = drift\n",
    "        self.dividend   = dividend\n",
    "        self.volatility = volatility\n",
    "        self.spot       = spot\n",
    "        self.nb_stocks  = nb_stocks\n",
    "        self.nb_paths   = nb_paths\n",
    "        self.nb_dates   = nb_dates\n",
    "        self.maturity   = maturity\n",
    "        self.dt         = maturity / nb_dates\n",
    "        self.df         = math.exp(-self.rate * self.dt)\n",
    "        self.return_var = False  # no variance process for plain BS\n",
    "\n",
    "    def generate_paths(self, nb_paths=None, return_dW=False, dW=None,\n",
    "                       X0=None, nb_dates=None):\n",
    "        \"\"\"Returns ndarray (nb_paths, nb_stocks, nb_dates+1).\n",
    "        Copied verbatim from stock_model.py::BlackScholes.generate_paths().\n",
    "        \"\"\"\n",
    "        nb_paths = nb_paths or self.nb_paths\n",
    "        nb_dates = nb_dates or self.nb_dates\n",
    "        spot_paths = np.empty((nb_paths, self.nb_stocks, nb_dates + 1))\n",
    "        if X0 is None:\n",
    "            spot_paths[:, :, 0] = self.spot\n",
    "        else:\n",
    "            spot_paths[:, :, 0] = X0\n",
    "        if dW is None:\n",
    "            random_numbers = np.random.normal(0, 1, (nb_paths, self.nb_stocks, nb_dates))\n",
    "            dW = random_numbers * np.sqrt(self.dt)\n",
    "        drift = self.drift\n",
    "        r = np.repeat(np.repeat(np.repeat(\n",
    "            np.reshape(drift, (-1, 1, 1)), nb_paths, axis=0),\n",
    "            self.nb_stocks, axis=1), nb_dates, axis=2)\n",
    "        sig = np.repeat(np.repeat(np.repeat(\n",
    "            np.reshape(self.volatility, (-1, 1, 1)), nb_paths, axis=0),\n",
    "            self.nb_stocks, axis=1), nb_dates, axis=2)\n",
    "        spot_paths[:, :, 1:] = np.repeat(\n",
    "            spot_paths[:, :, 0:1], nb_dates, axis=2) * np.exp(np.cumsum(\n",
    "            r * self.dt - (sig ** 2) * self.dt / 2 + sig * dW, axis=2))\n",
    "        if return_dW:\n",
    "            return spot_paths, None, dW\n",
    "        return spot_paths, None   # second value = var_paths (None for plain BS)\n",
    "\n",
    "\n",
    "# Instantiate and generate paths once — all three algorithms share these\n",
    "model = BlackScholes(\n",
    "    drift=RATE, volatility=VOLATILITY, nb_paths=NB_PATHS,\n",
    "    nb_stocks=1, nb_dates=NB_DATES, spot=SPOT,\n",
    "    maturity=MATURITY, dividend=DIVIDEND)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "stock_paths, var_paths = model.generate_paths()\n",
    "print(f'stock_paths shape: {stock_paths.shape}  →  (nb_paths, nb_stocks, nb_dates+1)')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "t_grid = np.linspace(0, MATURITY, NB_DATES + 1)\n",
    "axes[0].plot(t_grid, stock_paths[:200, 0, :].T, alpha=0.15, color='steelblue', lw=0.8)\n",
    "axes[0].axhline(STRIKE, color='red', lw=1.5, ls='--', label=f'K={STRIKE}')\n",
    "axes[0].set_title('Sample GBM Paths (first 200)'); axes[0].legend()\n",
    "axes[0].set_xlabel('Time'); axes[0].set_ylabel('Stock Price')\n",
    "axes[1].hist(stock_paths[:, 0, -1], bins=60, color='steelblue', edgecolor='white', alpha=0.8)\n",
    "axes[1].axvline(STRIKE, color='red', lw=1.5, ls='--', label=f'K={STRIKE}')\n",
    "axes[1].set_title('Terminal Price Distribution'); axes[1].legend()\n",
    "axes[1].set_xlabel('$S_T$'); axes[1].set_ylabel('Count')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 · Reference Price — CRR Binomial Tree\n",
    "\n",
    "Not from the repo — added as a numerical benchmark only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT from repo — benchmark only\n",
    "def binomial_american_put(S0, K, r, sigma, T, N=2000, dividend=0.0):\n",
    "    dt = T / N; u = np.exp(sigma * np.sqrt(dt)); d = 1.0 / u\n",
    "    p = (np.exp((r - dividend) * dt) - d) / (u - d)\n",
    "    disc = np.exp(-r * dt)\n",
    "    V = np.maximum(K - S0 * u**(N - 2*np.arange(N+1)), 0.0)\n",
    "    for i in range(N - 1, -1, -1):\n",
    "        S_i = S0 * u**(i - 2*np.arange(i+1))\n",
    "        V   = disc * (p * V[:i+1] + (1-p) * V[1:i+2])\n",
    "        V   = np.maximum(V, np.maximum(K - S_i, 0.0))\n",
    "    return float(V[0])\n",
    "\n",
    "ref_price = binomial_american_put(SPOT, STRIKE, RATE, VOLATILITY, MATURITY, N=2000)\n",
    "print(f'CRR Binomial (N=2000) reference price: {ref_price:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 · Base Pricer — `AmericanOptionPricer`\n",
    "\n",
    "**Source:** `backward_induction_pricer.py` → `AmericanOptionPricer`  \n",
    "`__init__`, `stop()`, and `price()` copied **verbatim** from lines 22–161.  \n",
    "Removed: `use_rnn`/`use_path` branches (not triggered for Markovian 1-stock put),  \n",
    "`configs.path_gen_seed` (replaced by `np.random.seed` at call site),  \n",
    "`price_upper_lower_bound`, `price_and_greeks`, and all Greek helper methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: backward_induction_pricer.py → AmericanOptionPricer\n",
    "# __init__, stop(), price() copied verbatim (lines 22-161).\n",
    "# Removed: use_rnn/use_path branches, configs seed, Greeks, upper/lower bound.\n",
    "class AmericanOptionPricer:\n",
    "    \"\"\"Computes the price of an American Option using backward recursion.\n",
    "    Source: backward_induction_pricer.py::AmericanOptionPricer\n",
    "    \"\"\"\n",
    "    def __init__(self, model, payoff, use_rnn=False, train_ITM_only=True,\n",
    "                 use_path=False, use_payoff_as_input=False):\n",
    "        self.model               = model\n",
    "        self.use_var             = model.return_var   # True for Heston; False for BS\n",
    "        self.payoff              = payoff\n",
    "        self.use_rnn             = use_rnn\n",
    "        self.use_path            = use_path\n",
    "        self.train_ITM_only      = train_ITM_only\n",
    "        self.use_payoff_as_input = use_payoff_as_input\n",
    "        self.which_weight        = 0\n",
    "\n",
    "    def calculate_continuation_value(\n",
    "            self, values, immediate_exercise_value, stock_paths_at_timestep):\n",
    "        \"\"\"Overridden by each algorithm subclass.\n",
    "        Source: backward_induction_pricer.py::AmericanOptionPricer.calculate_continuation_value()\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def stop(self, stock_values, immediate_exercise_values,\n",
    "             discounted_next_values, h=None, var_paths=None,\n",
    "             return_continuation_values=False):\n",
    "        \"\"\"Returns {0,1} vector: 1=stop (exercise), 0=continue.\n",
    "        Copied verbatim from backward_induction_pricer.py::AmericanOptionPricer.stop()\n",
    "        \"\"\"\n",
    "        stopping_rule = np.zeros(len(stock_values))\n",
    "        if self.use_var:\n",
    "            stock_values = np.concatenate([stock_values, var_paths], axis=1)\n",
    "        continuation_values = self.calculate_continuation_value(\n",
    "            discounted_next_values, immediate_exercise_values, stock_values)\n",
    "        if self.train_ITM_only:\n",
    "            which = (immediate_exercise_values > continuation_values) & \\\n",
    "                    (immediate_exercise_values > np.finfo(float).eps)\n",
    "        else:\n",
    "            which = immediate_exercise_values > continuation_values\n",
    "        stopping_rule[which] = 1\n",
    "        if return_continuation_values:\n",
    "            return stopping_rule, continuation_values\n",
    "        return stopping_rule\n",
    "\n",
    "    def price(self, stock_paths, var_paths, train_eval_split=2):\n",
    "        \"\"\"Compute American option price via backward recursion.\n",
    "        Copied verbatim from backward_induction_pricer.py::AmericanOptionPricer.price()\n",
    "        Signature change: stock_paths/var_paths passed in (pre-generated) so all\n",
    "        three algorithms can be compared on identical paths.\n",
    "        \"\"\"\n",
    "        model = self.model\n",
    "        payoffs = self.payoff(stock_paths)   # (nb_paths, nb_dates+1)\n",
    "        stock_paths_with_payoff = np.concatenate(\n",
    "            [stock_paths, np.expand_dims(payoffs, axis=1)], axis=1)\n",
    "        self.split = int(len(stock_paths) / train_eval_split)\n",
    "\n",
    "        # verbatim from original\n",
    "        disc_factor = math.exp(-model.rate * model.maturity / model.nb_dates)\n",
    "        immediate_exercise_value = self.payoff.eval(stock_paths[:, :, -1])\n",
    "        values = immediate_exercise_value\n",
    "\n",
    "        for i, date in enumerate(range(stock_paths.shape[2] - 2, 0, -1)):\n",
    "            self.which_weight = i\n",
    "            immediate_exercise_value = self.payoff.eval(stock_paths[:, :, date])\n",
    "            varp  = var_paths[:, :, date] if self.use_var else None\n",
    "            paths = (stock_paths_with_payoff[:, :, date]\n",
    "                     if self.use_payoff_as_input\n",
    "                     else stock_paths[:, :, date])\n",
    "            stopping_rule = self.stop(\n",
    "                paths, immediate_exercise_value,\n",
    "                values * disc_factor, h=None, var_paths=varp)\n",
    "            which = stopping_rule > 0.5\n",
    "            values[which]  = immediate_exercise_value[which]\n",
    "            values[~which] *= disc_factor\n",
    "\n",
    "        payoff_0 = self.payoff.eval(stock_paths[:, :, 0])[0]\n",
    "        # Use only eval half (self.split:) to avoid in-sample bias — verbatim from original\n",
    "        return max(payoff_0, np.mean(values[self.split:]) * disc_factor), 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 · Basis Functions — `BasisFunctions`\n",
    "\n",
    "**Source:** `algorithms/utils/basis_functions.py` (not uploaded)  \n",
    "Inlined from its usage in `regression.py::LeastSquares`:  \n",
    "`self.bf.nb_base_fcts` and `self.bf.base_fct(coeff, X, d2=True)`.  \n",
    "For 1 stock, degree-2 polynomial basis gives 3 features: `[1, S, S²]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: algorithms/utils/basis_functions.py (NOT UPLOADED — inlined from usage in regression.py)\n",
    "# regression.py::LeastSquares calls:\n",
    "#   self.bf = basis_functions.BasisFunctions(nb_stocks)\n",
    "#   self.bf.nb_base_fcts\n",
    "#   self.bf.base_fct(coeff, X[:,:], d2=True)\n",
    "class BasisFunctions:\n",
    "    \"\"\"Polynomial basis functions of degree <= 2 for LSM regression.\n",
    "    Source: algorithms/utils/basis_functions.py (inlined)\n",
    "    For nb_stocks=1: basis = [1, S, S^2]  →  nb_base_fcts = 3\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_stocks):\n",
    "        self.nb_stocks    = nb_stocks\n",
    "        # 1 constant + nb_stocks linear + nb_stocks*(nb_stocks+1)/2 quadratic\n",
    "        self.nb_base_fcts = 1 + nb_stocks + nb_stocks * (nb_stocks + 1) // 2\n",
    "\n",
    "    def base_fct(self, coeff, X, d2=True):\n",
    "        \"\"\"Evaluate coeff-th basis function on X (nb_paths, nb_stocks).\"\"\"\n",
    "        nb_paths, nb_stocks = X.shape\n",
    "        if coeff == 0:\n",
    "            return np.ones(nb_paths)            # constant\n",
    "        elif coeff <= nb_stocks:\n",
    "            return X[:, coeff - 1]              # linear: S_1, ..., S_d\n",
    "        else:\n",
    "            # quadratic cross terms: S_i * S_j  (i <= j)\n",
    "            idx   = coeff - nb_stocks - 1\n",
    "            pairs = [(i, j) for i in range(nb_stocks) for j in range(i, nb_stocks)]\n",
    "            i, j  = pairs[idx]\n",
    "            return X[:, i] * X[:, j]\n",
    "\n",
    "bf = BasisFunctions(nb_stocks=1)\n",
    "print(f'nb_base_fcts (1 stock): {bf.nb_base_fcts}  →  [1, S, S²]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 · Algorithm 1 — LSM\n",
    "\n",
    "**Source:**\n",
    "- `regression.py` → `LeastSquares.calculate_regression()` — **verbatim**\n",
    "- `LSM.py` → `LeastSquaresPricer.__init__()` + `calculate_continuation_value()` — **verbatim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: regression.py → LeastSquares (verbatim)\n",
    "class LeastSquares:\n",
    "    \"\"\"OLS on polynomial basis functions.\n",
    "    Source: regression.py::LeastSquares\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_stocks):\n",
    "        self.nb_stocks = nb_stocks\n",
    "        self.bf = BasisFunctions(self.nb_stocks)\n",
    "\n",
    "    def calculate_regression(self, X, Y, in_the_money, in_the_money_all):\n",
    "        \"\"\"Copied verbatim from regression.py::LeastSquares.calculate_regression()\"\"\"\n",
    "        nb_paths, nb_stocks = X.shape\n",
    "        reg_vect_mat = np.empty((nb_paths, self.bf.nb_base_fcts))\n",
    "        for coeff in range(self.bf.nb_base_fcts):\n",
    "            reg_vect_mat[:, coeff] = self.bf.base_fct(coeff, X[:, :], d2=True)\n",
    "        coefficients = np.linalg.lstsq(\n",
    "            reg_vect_mat[in_the_money[0]], Y[in_the_money[0]], rcond=None)\n",
    "        continuation_values = np.dot(\n",
    "            reg_vect_mat[in_the_money_all[0]], coefficients[0])\n",
    "        return continuation_values\n",
    "\n",
    "\n",
    "# SOURCE: LSM.py → LeastSquaresPricer (verbatim)\n",
    "class LeastSquaresPricer(AmericanOptionPricer):\n",
    "    \"\"\"American option pricer — Least-Square Monte Carlo.\n",
    "    Source: LSM.py::LeastSquaresPricer\n",
    "    \"\"\"\n",
    "    def __init__(self, model, payoff, nb_epochs=None, nb_batches=None,\n",
    "                 train_ITM_only=True, use_payoff_as_input=False):\n",
    "        super().__init__(model, payoff, train_ITM_only=train_ITM_only,\n",
    "                         use_payoff_as_input=use_payoff_as_input)\n",
    "        self.regression = LeastSquares(\n",
    "            model.nb_stocks * (1 + self.use_var) + self.use_payoff_as_input * 1)\n",
    "\n",
    "    def calculate_continuation_value(\n",
    "            self, values, immediate_exercise_value, stock_paths_at_timestep):\n",
    "        \"\"\"Copied verbatim from LSM.py::LeastSquaresPricer.calculate_continuation_value()\"\"\"\n",
    "        if self.train_ITM_only:\n",
    "            in_the_money     = np.where(immediate_exercise_value[:self.split] > 0)\n",
    "            in_the_money_all = np.where(immediate_exercise_value > 0)\n",
    "        else:\n",
    "            in_the_money     = np.where(immediate_exercise_value[:self.split] < np.inf)\n",
    "            in_the_money_all = np.where(immediate_exercise_value < np.inf)\n",
    "        return_values = np.zeros(stock_paths_at_timestep.shape[0])\n",
    "        return_values[in_the_money_all[0]] = self.regression.calculate_regression(\n",
    "            stock_paths_at_timestep, values, in_the_money, in_the_money_all)\n",
    "        return return_values\n",
    "\n",
    "\n",
    "# Run\n",
    "lsm_pricer = LeastSquaresPricer(model, payoff, train_ITM_only=True)\n",
    "t0 = time.time()\n",
    "price_lsm, _ = lsm_pricer.price(stock_paths, var_paths)\n",
    "t_lsm = time.time() - t0\n",
    "print(f'LSM  price={price_lsm:.4f}  ref={ref_price:.4f}  err={price_lsm-ref_price:+.4f}  [{t_lsm:.2f}s]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 · Algorithm 2 — RLSM\n",
    "\n",
    "**Source:**\n",
    "- `algorithms/utils/randomized_neural_networks.py` → `Reservoir2` (not uploaded — inlined from usage in `regression.py`)\n",
    "- `regression.py` → `ReservoirLeastSquares2.calculate_regression()` — **verbatim** (already used PyTorch)\n",
    "- `RLSM.py` → `ReservoirLeastSquarePricerFastTanh` — **verbatim**\n",
    "\n",
    "`Reservoir2` is a `torch.nn.Module` with **frozen random weights** (no gradient updates ever).  \n",
    "Only the output layer (solved by OLS) is fitted per time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: algorithms/utils/randomized_neural_networks.py → Reservoir2\n",
    "# NOT UPLOADED — inlined from its usage in regression.py::ReservoirLeastSquares2:\n",
    "#   self.reservoir = randomized_neural_networks.Reservoir2(\n",
    "#       hidden_size, self.state_size, factors=factors, activation=activation)\n",
    "#   self.reservoir(X)   ← called as a nn.Module in calculate_regression()\n",
    "class Reservoir2(nn.Module):\n",
    "    \"\"\"Frozen random single-hidden-layer network (ELM).\n",
    "    Source: algorithms/utils/randomized_neural_networks.py::Reservoir2 (inlined)\n",
    "    Weights drawn once, frozen forever. Only OLS output layer changes per step.\n",
    "    factors[0] controls weight init scale (matches RLSM.py usage: factors=(1.,)).\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, state_size, factors=(1.,), activation=nn.Tanh()):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.linear     = nn.Linear(state_size, hidden_size)\n",
    "        self._init_and_freeze(factors)\n",
    "\n",
    "    def _init_and_freeze(self, factors):\n",
    "        scale = factors[0] if len(factors) > 0 else 1.0\n",
    "        nn.init.normal_(self.linear.weight, mean=0.0, std=scale)\n",
    "        nn.init.uniform_(self.linear.bias, -1.0, 1.0)\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad_(False)   # FROZEN — never updated\n",
    "\n",
    "    def init(self):\n",
    "        \"\"\"Re-randomise weights (used when reinit=True in ReservoirLeastSquares2).\"\"\"\n",
    "        self._init_and_freeze((1.,))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.linear(x))\n",
    "\n",
    "\n",
    "# SOURCE: regression.py → ReservoirLeastSquares2 (verbatim)\n",
    "# Already used PyTorch in the original — no TF conversion needed.\n",
    "class ReservoirLeastSquares2:\n",
    "    \"\"\"OLS regression on frozen random neural network features.\n",
    "    Source: regression.py::ReservoirLeastSquares2\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, hidden_size=10, factors=(1.,),\n",
    "                 activation=nn.LeakyReLU(0.5), reinit=False):\n",
    "        self.nb_base_fcts = hidden_size + 1\n",
    "        self.state_size   = state_size\n",
    "        self.reinit       = reinit\n",
    "        self.reservoir    = Reservoir2(\n",
    "            hidden_size, self.state_size, factors=factors, activation=activation)\n",
    "\n",
    "    def calculate_regression(self, X_unsorted, Y, in_the_money, in_the_money_all,\n",
    "                              coefficients=None, return_coefficients=False):\n",
    "        \"\"\"Copied verbatim from regression.py::ReservoirLeastSquares2.calculate_regression()\"\"\"\n",
    "        if self.reinit:\n",
    "            self.reservoir.init()\n",
    "        X = torch.from_numpy(X_unsorted).type(torch.float32)\n",
    "        reg_input = np.concatenate(\n",
    "            [self.reservoir(X).detach().numpy(), np.ones((len(X), 1))], axis=1)\n",
    "        if coefficients is None:\n",
    "            coefficients = np.linalg.lstsq(\n",
    "                reg_input[in_the_money[0]], Y[in_the_money[0]], rcond=None)\n",
    "        continuation_values = np.dot(reg_input[in_the_money_all[0]], coefficients[0])\n",
    "        if return_coefficients:\n",
    "            return continuation_values, coefficients\n",
    "        return continuation_values\n",
    "\n",
    "\n",
    "# SOURCE: RLSM.py → ReservoirLeastSquarePricerFastTanh (verbatim)\n",
    "class ReservoirLeastSquarePricerFastTanh(LeastSquaresPricer):\n",
    "    \"\"\"RLSM pricer with Tanh activation on the frozen reservoir.\n",
    "    Source: RLSM.py::ReservoirLeastSquarePricerFastTanh\n",
    "    Inherits calculate_continuation_value() from LeastSquaresPricer unchanged;\n",
    "    only self.regression is swapped to ReservoirLeastSquares2(activation=Tanh).\n",
    "    \"\"\"\n",
    "    def __init__(self, model, payoff, hidden_size=10, factors=(1.,),\n",
    "                 nb_epochs=None, nb_batches=None, train_ITM_only=True,\n",
    "                 use_payoff_as_input=False):\n",
    "        super().__init__(model, payoff, train_ITM_only=train_ITM_only,\n",
    "                         use_payoff_as_input=use_payoff_as_input)\n",
    "        if hidden_size < 0:\n",
    "            hidden_size = 50 + abs(hidden_size) * model.nb_stocks\n",
    "        state_size = model.nb_stocks * (1 + self.use_var) + self.use_payoff_as_input * 1\n",
    "        self.regression = ReservoirLeastSquares2(\n",
    "            state_size, hidden_size, factors=factors, activation=nn.Tanh())\n",
    "\n",
    "\n",
    "# Run\n",
    "rlsm_pricer = ReservoirLeastSquarePricerFastTanh(\n",
    "    model, payoff, hidden_size=100, factors=(1.,), train_ITM_only=True)\n",
    "t0 = time.time()\n",
    "price_rlsm, _ = rlsm_pricer.price(stock_paths, var_paths)\n",
    "t_rlsm = time.time() - t0\n",
    "print(f'RLSM price={price_rlsm:.4f}  ref={ref_price:.4f}  err={price_rlsm-ref_price:+.4f}  [{t_rlsm:.2f}s]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 · Algorithm 3 — NLSM\n",
    "\n",
    "**Source:**\n",
    "- `algorithms/utils/neural_networks.py` → `NetworkNLSM` (not uploaded) — **TF/Keras → PyTorch conversion**\n",
    "- `NLSM.py` → `init_weights`, `NeuralRegression`, `NeuralNetworkPricer` — **verbatim**  \n",
    "  (`train_network` and `evaluate_network` already used PyTorch in the original)\n",
    "\n",
    "**TF → PyTorch details:**  \n",
    "The original `NetworkNLSM` was `tf.keras.Sequential([Dense(hidden), Tanh, Dense(hidden), Tanh, Dense(1)])`.  \n",
    "Replaced with `torch.nn.Sequential` using identical layer sizes and activations.  \n",
    "Called with `.double()` to match the original Keras float64 default (see `NeuralRegression.__init__`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: algorithms/utils/neural_networks.py → NetworkNLSM\n",
    "# NOT UPLOADED. Original was TensorFlow/Keras. CONVERTED TO PyTorch.\n",
    "# Architecture unchanged: Linear→Tanh→Linear→Tanh→Linear(1)\n",
    "class NetworkNLSM(nn.Module):\n",
    "    \"\"\"MLP for NLSM continuation value.\n",
    "    Source: algorithms/utils/neural_networks.py::NetworkNLSM\n",
    "    CONVERTED from TensorFlow/Keras to PyTorch.\n",
    "    Architecture unchanged: two hidden layers with Tanh, output dim=1.\n",
    "    Called as .double() in NeuralRegression to match original Keras float64.\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_stocks, hidden_size=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(nb_stocks, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)   # (N, 1)\n",
    "\n",
    "\n",
    "# SOURCE: NLSM.py → init_weights (verbatim)\n",
    "def init_weights(m):\n",
    "    \"\"\"Xavier uniform init. Source: NLSM.py::init_weights\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "# SOURCE: NLSM.py → NeuralRegression (verbatim)\n",
    "# train_network() and evaluate_network() already used PyTorch in the original.\n",
    "# Only change: NetworkNLSM is now the PyTorch version above.\n",
    "class NeuralRegression:\n",
    "    \"\"\"Train/evaluate the MLP for the NLSM continuation value.\n",
    "    Source: NLSM.py::NeuralRegression (verbatim)\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_stocks, nb_paths, hidden_size=10,\n",
    "                 nb_iters=20, batch_size=2000):\n",
    "        self.batch_size     = batch_size\n",
    "        self.nb_stocks      = nb_stocks\n",
    "        self.nb_paths       = nb_paths\n",
    "        self.nb_iters       = nb_iters\n",
    "        # .double() matches original Keras float64 weights\n",
    "        self.neural_network = NetworkNLSM(self.nb_stocks, hidden_size=hidden_size).double()\n",
    "        self.neural_network.apply(init_weights)\n",
    "\n",
    "    def train_network(self, X_inputs, Y_labels):\n",
    "        \"\"\"Copied verbatim from NLSM.py::NeuralRegression.train_network()\"\"\"\n",
    "        optimizer = optim.Adam(self.neural_network.parameters())\n",
    "        X_inputs  = torch.from_numpy(X_inputs).double()\n",
    "        Y_labels  = torch.from_numpy(Y_labels).double().view(len(Y_labels), 1)\n",
    "        self.neural_network.train(True)\n",
    "        for iteration in range(self.nb_iters):\n",
    "            for batch in tdata.BatchSampler(\n",
    "                    tdata.RandomSampler(range(len(X_inputs)), replacement=False),\n",
    "                    batch_size=self.batch_size, drop_last=False):\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    outputs = self.neural_network(X_inputs[batch])\n",
    "                    loss    = nn.MSELoss(reduction='mean')(outputs, Y_labels[batch])\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "    def evaluate_network(self, X_inputs):\n",
    "        \"\"\"Copied verbatim from NLSM.py::NeuralRegression.evaluate_network()\"\"\"\n",
    "        self.neural_network.train(False)\n",
    "        X_inputs = torch.from_numpy(X_inputs).double()\n",
    "        outputs  = self.neural_network(X_inputs)\n",
    "        return outputs.view(len(X_inputs)).detach().numpy()\n",
    "\n",
    "\n",
    "# SOURCE: NLSM.py → NeuralNetworkPricer (verbatim)\n",
    "class NeuralNetworkPricer(AmericanOptionPricer):\n",
    "    \"\"\"American option pricer — Neural Least-Square Monte Carlo.\n",
    "    Source: NLSM.py::NeuralNetworkPricer (verbatim)\n",
    "    The only change: NeuralRegression uses PyTorch NetworkNLSM instead of Keras.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, payoff, nb_epochs=20, nb_batches=None,\n",
    "                 hidden_size=10, train_ITM_only=True, use_payoff_as_input=False):\n",
    "        del nb_batches\n",
    "        super().__init__(model, payoff, train_ITM_only=train_ITM_only,\n",
    "                         use_payoff_as_input=use_payoff_as_input)\n",
    "        self.neural_regression = NeuralRegression(\n",
    "            model.nb_stocks * (1 + self.use_var) + self.use_payoff_as_input * 1,\n",
    "            model.nb_paths, hidden_size=hidden_size, nb_iters=nb_epochs)\n",
    "\n",
    "    def calculate_continuation_value(\n",
    "            self, values, immediate_exercise_value, stock_paths_at_timestep):\n",
    "        \"\"\"Copied verbatim from NLSM.py::NeuralNetworkPricer.calculate_continuation_value()\"\"\"\n",
    "        inputs = stock_paths_at_timestep\n",
    "        if self.train_ITM_only:\n",
    "            in_the_money     = np.where(immediate_exercise_value[:self.split] > 0)\n",
    "            in_the_money_all = np.where(immediate_exercise_value > 0)\n",
    "        else:\n",
    "            in_the_money     = np.where(immediate_exercise_value[:self.split] < np.inf)\n",
    "            in_the_money_all = np.where(immediate_exercise_value < np.inf)\n",
    "        continuation_values = np.zeros(stock_paths_at_timestep.shape[0])\n",
    "        self.neural_regression.train_network(\n",
    "            inputs[in_the_money[0]], values[in_the_money[0]])\n",
    "        continuation_values[in_the_money_all[0]] = \\\n",
    "            self.neural_regression.evaluate_network(inputs[in_the_money_all[0]])\n",
    "        return continuation_values\n",
    "\n",
    "\n",
    "# Run\n",
    "nlsm_pricer = NeuralNetworkPricer(\n",
    "    model, payoff, nb_epochs=20, hidden_size=50, train_ITM_only=True)\n",
    "t0 = time.time()\n",
    "price_nlsm, _ = nlsm_pricer.price(stock_paths, var_paths)\n",
    "t_nlsm = time.time() - t0\n",
    "print(f'NLSM price={price_nlsm:.4f}  ref={ref_price:.4f}  err={price_nlsm-ref_price:+.4f}  [{t_nlsm:.2f}s]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 · Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'CRR Binomial (ref)': dict(price=ref_price,  time=None),\n",
    "    'LSM':                dict(price=price_lsm,  time=t_lsm),\n",
    "    'RLSM':               dict(price=price_rlsm, time=t_rlsm),\n",
    "    'NLSM':               dict(price=price_nlsm, time=t_nlsm),\n",
    "}\n",
    "print(f\"\\n{'='*62}\")\n",
    "print(f\"  American Put  S0={SPOT} K={STRIKE} r={RATE} σ={VOLATILITY} T={MATURITY}\")\n",
    "print(f\"  Paths={NB_PATHS:,}   Dates={NB_DATES}\")\n",
    "print(f\"{'='*62}\")\n",
    "print(f\"  {'Algorithm':<22} {'Price':>8}  {'Error':>8}  {'Time':>8}\")\n",
    "print(f\"  {'-'*52}\")\n",
    "for name, v in results.items():\n",
    "    err   = f\"{v['price']-ref_price:+.4f}\" if v['time'] is not None else '   ref  '\n",
    "    t_str = f\"{v['time']:.2f}s\"            if v['time'] is not None else '   —    '\n",
    "    print(f\"  {name:<22} {v['price']:>8.4f}  {err:>8}  {t_str:>8}\")\n",
    "print(f\"{'='*62}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos  = ['LSM', 'RLSM', 'NLSM']\n",
    "prices = [price_lsm, price_rlsm, price_nlsm]\n",
    "times  = [t_lsm,     t_rlsm,     t_nlsm]\n",
    "colors = ['#2196F3', '#4CAF50',  '#FF9800']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax = axes[0]\n",
    "bars = ax.bar(algos, prices, color=colors, width=0.5, edgecolor='black', lw=0.8)\n",
    "ax.axhline(ref_price, color='red', lw=2, ls='--', label=f'CRR ref={ref_price:.4f}')\n",
    "ax.set_ylim(ref_price*0.97, ref_price*1.03)\n",
    "for bar, p in zip(bars, prices):\n",
    "    ax.text(bar.get_x()+bar.get_width()/2, p+0.002, f'{p:.4f}',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Price Estimate vs Reference', fontsize=13)\n",
    "ax.set_ylabel('Price'); ax.legend(fontsize=11); ax.grid(axis='y', alpha=0.4)\n",
    "ax = axes[1]\n",
    "bars2 = ax.bar(algos, times, color=colors, width=0.5, edgecolor='black', lw=0.8)\n",
    "for bar, t in zip(bars2, times):\n",
    "    ax.text(bar.get_x()+bar.get_width()/2, t*1.02, f'{t:.2f}s',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Wall-Clock Runtime', fontsize=13)\n",
    "ax.set_ylabel('Seconds'); ax.grid(axis='y', alpha=0.4)\n",
    "plt.suptitle(f'American Put  S0={SPOT} K={STRIKE} σ={VOLATILITY} r={RATE} T={MATURITY}'\n",
    "             f'  |  {NB_PATHS:,} paths · {NB_DATES} dates', fontsize=11, y=1.01)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 · Sensitivity — Price Convergence vs Number of Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_counts  = [2_000, 5_000, 10_000, 25_000, 50_000]\n",
    "conv_results = {a: [] for a in algos}\n",
    "\n",
    "for n in path_counts:\n",
    "    print(f'n={n:>6,}', end='  ')\n",
    "    m_n = BlackScholes(drift=RATE, volatility=VOLATILITY, nb_paths=n,\n",
    "                       nb_stocks=1, nb_dates=NB_DATES, spot=SPOT,\n",
    "                       maturity=MATURITY, dividend=DIVIDEND)\n",
    "    np.random.seed(SEED)\n",
    "    sp_n, vp_n = m_n.generate_paths()\n",
    "\n",
    "    p_lsm,  _ = LeastSquaresPricer(m_n, payoff).price(sp_n, vp_n)\n",
    "    p_rlsm, _ = ReservoirLeastSquarePricerFastTanh(\n",
    "                    m_n, payoff, hidden_size=100).price(sp_n, vp_n)\n",
    "    p_nlsm, _ = NeuralNetworkPricer(\n",
    "                    m_n, payoff, nb_epochs=20, hidden_size=50).price(sp_n, vp_n)\n",
    "    conv_results['LSM'].append(p_lsm)\n",
    "    conv_results['RLSM'].append(p_rlsm)\n",
    "    conv_results['NLSM'].append(p_nlsm)\n",
    "    print(f'LSM={p_lsm:.4f}  RLSM={p_rlsm:.4f}  NLSM={p_nlsm:.4f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "for alg, col in zip(algos, colors):\n",
    "    ax.plot(path_counts, conv_results[alg], 'o-', color=col, lw=2, ms=7, label=alg)\n",
    "ax.axhline(ref_price, color='red', lw=2, ls='--', label=f'CRR ref={ref_price:.4f}')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Number of Paths (log scale)', fontsize=12)\n",
    "ax.set_ylabel('Price Estimate', fontsize=12)\n",
    "ax.set_title('Convergence vs Number of Paths', fontsize=13)\n",
    "ax.legend(fontsize=11); ax.grid(alpha=0.4)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 · Sensitivity — RLSM Reservoir Size\n",
    "\n",
    "A key finding of Herrera et al. (2023): RLSM price quality improves rapidly with more random neurons.  \n",
    "The cost increase is minimal — there is no backprop, only a larger closed-form OLS solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes  = [10, 25, 50, 100, 200, 400]\n",
    "rlsm_prices_h = []\n",
    "rlsm_times_h  = []\n",
    "\n",
    "for h in hidden_sizes:\n",
    "    pr = ReservoirLeastSquarePricerFastTanh(model, payoff, hidden_size=h)\n",
    "    t0 = time.time()\n",
    "    p, _ = pr.price(stock_paths, var_paths)\n",
    "    t_   = time.time() - t0\n",
    "    rlsm_prices_h.append(p); rlsm_times_h.append(t_)\n",
    "    print(f'  H={h:4d}  price={p:.4f}  err={p-ref_price:+.4f}  [{t_:.2f}s]')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "ax = axes[0]\n",
    "ax.plot(hidden_sizes, rlsm_prices_h, 'o-', color='#4CAF50', lw=2, ms=8)\n",
    "ax.axhline(ref_price, color='red', lw=2, ls='--', label=f'CRR ref={ref_price:.4f}')\n",
    "ax.set_xlabel('Hidden Units (H)', fontsize=12); ax.set_ylabel('Price', fontsize=12)\n",
    "ax.set_title('RLSM: Accuracy vs Reservoir Size', fontsize=13)\n",
    "ax.legend(fontsize=11); ax.grid(alpha=0.4)\n",
    "ax = axes[1]\n",
    "ax.plot(hidden_sizes, rlsm_times_h, 's-', color='#4CAF50', lw=2, ms=8)\n",
    "ax.set_xlabel('Hidden Units (H)', fontsize=12); ax.set_ylabel('Seconds', fontsize=12)\n",
    "ax.set_title('RLSM: Runtime vs Reservoir Size', fontsize=13)\n",
    "ax.grid(alpha=0.4)\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
